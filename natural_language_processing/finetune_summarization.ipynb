{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Summarization Model\n",
    "\n",
    "Now let's see how we can use `HuggingFace` to train a summarization model on a new dataset. We'll use the SAMSum dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset samsum (/home/alex/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23f8246fcb9e4f2097e2ce6041e9ea1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split lengths: [14732, 819, 818]\n",
      "Features: ['id', 'dialogue', 'summary']\n",
      "\n",
      "Dialogue:\n",
      "Hannah: Hey, do you have Betty's number?\n",
      "Amanda: Lemme check\n",
      "Hannah: <file_gif>\n",
      "Amanda: Sorry, can't find it.\n",
      "Amanda: Ask Larry\n",
      "Amanda: He called her last time we were at the park together\n",
      "Hannah: I don't know him well\n",
      "Hannah: <file_gif>\n",
      "Amanda: Don't be shy, he's very nice\n",
      "Hannah: If you say so..\n",
      "Hannah: I'd rather you texted him\n",
      "Amanda: Just text him ðŸ™‚\n",
      "Hannah: Urgh.. Alright\n",
      "Hannah: Bye\n",
      "Amanda: Bye bye\n",
      "\n",
      "Summary\n",
      "Hannah needs Betty's number but Amanda doesn't have it. She needs to contact Larry.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "dataset_samsum = load_dataset(\"samsum\")\n",
    "split_lengths = [len(dataset_samsum[split]) for split in dataset_samsum]\n",
    "\n",
    "print(f\"Split lengths: {split_lengths}\")\n",
    "print(f\"Features: {dataset_samsum['train'].column_names}\")\n",
    "print(f\"\\nDialogue:\")\n",
    "print(dataset_samsum[\"test\"][0][\"dialogue\"])\n",
    "print(\"\\nSummary\")\n",
    "print(dataset_samsum[\"test\"][0][\"summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-25 21:15:54.103802: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Your max_length is set to 128, but you input_length is only 122. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=61)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      "Amanda: Ask Larry Amanda: He called her last time we were at the park together.\n",
      "Hannah: I'd rather you texted him.\n",
      "Amanda: Just text him .\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Evaluate this using PEGASUS\n",
    "pipe = pipeline(\"summarization\", model=\"google/pegasus-cnn_dailymail\", framework='pt')\n",
    "pipe_out = pipe(dataset_samsum[\"test\"][0][\"dialogue\"])\n",
    "print(\"Summary:\")\n",
    "print(pipe_out[0][\"summary_text\"].replace(\" .<n>\", \".\\n\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the entire test set\n",
    "\n",
    "We will need a way to compare the baseline PEGASUS model to the finetuned version. We'll create an evaluation loop for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def chunks(list_of_elements, batch_size):\n",
    "    for i in range(0, len(list_of_elements), batch_size):\n",
    "        yield list_of_elements[i : i + batch_size]\n",
    "\n",
    "def evaluate_summaries(dataset, metric, model, tokenizer,\n",
    "                       batch_size=16, device=device,\n",
    "                       column_text=\"article\", column_summary=\"highlights\"):\n",
    "    article_batches = list(chunks(dataset[column_text], batch_size))\n",
    "    target_batches = list(chunks(dataset[column_summary], batch_size))\n",
    "\n",
    "    for article_batch, target_batch in tqdm(\n",
    "        zip(article_batches, target_batches), total=len(article_batches)):\n",
    "\n",
    "        inputs = tokenizer(article_batch, max_length=1024, truncation=True,\n",
    "                        padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "        summaries = model.generate(input_ids=inputs[\"input_ids\"].to(device),\n",
    "                                   attention_mask=inputs[\"attention_mask\"].to(device),\n",
    "                                   length_penalty=0.8, num_beams=8, max_length=128)\n",
    "\n",
    "        decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True,\n",
    "                                              clean_up_tokenization_spaces=True)\n",
    "                             for s in summaries]\n",
    "\n",
    "        decoded_summaries = [d.replace(\"<n>\", \" \") for d in decoded_summaries]\n",
    "        \n",
    "    return metric.compute(predictions=decoded_summaries, references=target_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-25 21:23:15.223026: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Load the model directly\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "model_ckpt = \"ainize/bart-base-cnn\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 103/103 [04:32<00:00,  2.64s/it]\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "score = evaluate_summaries(dataset_samsum[\"test\"], rouge_metric, model,\n",
    "                           tokenizer, column_text=\"dialogue\",\n",
    "                           column_summary=\"summary\", batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>rougeLsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bart</th>\n",
       "      <td>0.383839</td>\n",
       "      <td>0.234127</td>\n",
       "      <td>0.347803</td>\n",
       "      <td>0.383839</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        rouge1    rouge2    rougeL  rougeLsum\n",
       "bart  0.383839  0.234127  0.347803   0.383839"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(score, index=[\"bart\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to fine tune this model, we need to be able to tokenize the data. We can also limit the lengths of each dialogue and summary to 1024 and 128, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5105be9987ce4156abf6ea545d0ece8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14732 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/anaconda3/envs/pytorch2/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3546: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09818c10ecc24da2a5cf5be4a4397eea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/819 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a09e39a2fcb4af08dce11ec44195e25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/818 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def convert_examples_to_features(example_batch):\n",
    "    input_encodings = tokenizer(example_batch[\"dialogue\"], truncation=True,\n",
    "                                max_length=1024)\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        target_encodings = tokenizer(example_batch[\"summary\"], max_length=128,\n",
    "                                     truncation=True)\n",
    "\n",
    "    return {\"input_ids\": input_encodings[\"input_ids\"],\n",
    "            \"attention_mask\": input_encodings[\"attention_mask\"],\n",
    "            \"labels\": target_encodings[\"input_ids\"]}\n",
    "\n",
    "dataset_samsum_pt = dataset_samsum.map(convert_examples_to_features,\n",
    "                                       batched=True)\n",
    "\n",
    "columns = [\"input_ids\", \"labels\", \"attention_mask\"]\n",
    "dataset_samsum_pt.set_format(type=\"torch\", columns=columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing a batch of data\n",
    "\n",
    "When training `seq2seq` models, we need to apply \"teacher forcing\". The encoder will receive input tokens using the labels shifted by one as well as the encoder output. The prediction is then compared to the shifted labels to calculate the loss. To clarify, the decoder only sees the previous ground truth labels.\n",
    "\n",
    "`HuggingFace` provides a `DataCollatorForSeq2Seq` class that handles this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "seq2seq_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# Gradient accumulation saves memory by updating the model only every X batches\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"bart-samsum\", num_train_epochs=1, warmup_steps=500,\n",
    "    per_device_train_batch_size=1, per_device_eval_batch_size=1,\n",
    "    weight_decay=0.01, logging_steps=10, push_to_hub=False,\n",
    "    evaluation_strategy=\"steps\", eval_steps=500, save_steps=1e6,\n",
    "    gradient_accumulation_steps=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: dialogue, summary, id. If dialogue, summary, id are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "/home/alex/anaconda3/envs/pytorch2/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 14732\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 16\n",
      "  Total optimization steps = 920\n",
      "  Number of trainable parameters = 139420416\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33majdillhoff\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/alex/dev/teaching/CSE6363/natural_language_processing/wandb/run-20230425_213928-d01c6jmr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ajdillhoff/huggingface/runs/d01c6jmr' target=\"_blank\">bart-samsum</a></strong> to <a href='https://wandb.ai/ajdillhoff/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ajdillhoff/huggingface' target=\"_blank\">https://wandb.ai/ajdillhoff/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ajdillhoff/huggingface/runs/d01c6jmr' target=\"_blank\">https://wandb.ai/ajdillhoff/huggingface/runs/d01c6jmr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c6617ee37d946b6a8271965d6abc12f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/920 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.9587, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.01}\n",
      "{'loss': 2.7171, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.02}\n",
      "{'loss': 2.5365, 'learning_rate': 3e-06, 'epoch': 0.03}\n",
      "{'loss': 2.3373, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.04}\n",
      "{'loss': 2.245, 'learning_rate': 5e-06, 'epoch': 0.05}\n",
      "{'loss': 2.1459, 'learning_rate': 6e-06, 'epoch': 0.07}\n",
      "{'loss': 2.0281, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.08}\n",
      "{'loss': 1.916, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.09}\n",
      "{'loss': 2.0147, 'learning_rate': 9e-06, 'epoch': 0.1}\n",
      "{'loss': 1.9108, 'learning_rate': 1e-05, 'epoch': 0.11}\n",
      "{'loss': 1.968, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.12}\n",
      "{'loss': 1.8644, 'learning_rate': 1.2e-05, 'epoch': 0.13}\n",
      "{'loss': 1.8956, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.14}\n",
      "{'loss': 1.8825, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.15}\n",
      "{'loss': 1.8367, 'learning_rate': 1.5e-05, 'epoch': 0.16}\n",
      "{'loss': 1.9063, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.17}\n",
      "{'loss': 1.7779, 'learning_rate': 1.7000000000000003e-05, 'epoch': 0.18}\n",
      "{'loss': 1.898, 'learning_rate': 1.8e-05, 'epoch': 0.2}\n",
      "{'loss': 1.8972, 'learning_rate': 1.9e-05, 'epoch': 0.21}\n",
      "{'loss': 1.8138, 'learning_rate': 2e-05, 'epoch': 0.22}\n",
      "{'loss': 1.8477, 'learning_rate': 2.1e-05, 'epoch': 0.23}\n",
      "{'loss': 1.8053, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.24}\n",
      "{'loss': 1.7627, 'learning_rate': 2.3000000000000003e-05, 'epoch': 0.25}\n",
      "{'loss': 1.8004, 'learning_rate': 2.4e-05, 'epoch': 0.26}\n",
      "{'loss': 1.7135, 'learning_rate': 2.5e-05, 'epoch': 0.27}\n",
      "{'loss': 1.8897, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.28}\n",
      "{'loss': 1.6728, 'learning_rate': 2.7000000000000002e-05, 'epoch': 0.29}\n",
      "{'loss': 1.7494, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.3}\n",
      "{'loss': 1.8096, 'learning_rate': 2.9e-05, 'epoch': 0.31}\n",
      "{'loss': 1.7087, 'learning_rate': 3e-05, 'epoch': 0.33}\n",
      "{'loss': 1.8226, 'learning_rate': 3.1e-05, 'epoch': 0.34}\n",
      "{'loss': 1.665, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.35}\n",
      "{'loss': 1.828, 'learning_rate': 3.3e-05, 'epoch': 0.36}\n",
      "{'loss': 1.7372, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.37}\n",
      "{'loss': 1.8032, 'learning_rate': 3.5e-05, 'epoch': 0.38}\n",
      "{'loss': 1.7394, 'learning_rate': 3.6e-05, 'epoch': 0.39}\n",
      "{'loss': 1.7557, 'learning_rate': 3.7e-05, 'epoch': 0.4}\n",
      "{'loss': 1.8106, 'learning_rate': 3.8e-05, 'epoch': 0.41}\n",
      "{'loss': 1.7554, 'learning_rate': 3.9000000000000006e-05, 'epoch': 0.42}\n",
      "{'loss': 1.7552, 'learning_rate': 4e-05, 'epoch': 0.43}\n",
      "{'loss': 1.772, 'learning_rate': 4.1e-05, 'epoch': 0.45}\n",
      "{'loss': 1.7465, 'learning_rate': 4.2e-05, 'epoch': 0.46}\n",
      "{'loss': 1.6667, 'learning_rate': 4.3e-05, 'epoch': 0.47}\n",
      "{'loss': 1.7713, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.48}\n",
      "{'loss': 1.7338, 'learning_rate': 4.5e-05, 'epoch': 0.49}\n",
      "{'loss': 1.7617, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.5}\n",
      "{'loss': 1.6181, 'learning_rate': 4.7e-05, 'epoch': 0.51}\n",
      "{'loss': 1.7016, 'learning_rate': 4.8e-05, 'epoch': 0.52}\n",
      "{'loss': 1.6852, 'learning_rate': 4.9e-05, 'epoch': 0.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: dialogue, summary, id. If dialogue, summary, id are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 818\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7254, 'learning_rate': 5e-05, 'epoch': 0.54}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93b9ef3a05e84ef4869a959a7bcc13ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/818 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5479345321655273, 'eval_runtime': 11.685, 'eval_samples_per_second': 70.004, 'eval_steps_per_second': 70.004, 'epoch': 0.54}\n",
      "{'loss': 1.8183, 'learning_rate': 4.880952380952381e-05, 'epoch': 0.55}\n",
      "{'loss': 1.7176, 'learning_rate': 4.761904761904762e-05, 'epoch': 0.56}\n",
      "{'loss': 1.7989, 'learning_rate': 4.642857142857143e-05, 'epoch': 0.58}\n",
      "{'loss': 1.7256, 'learning_rate': 4.523809523809524e-05, 'epoch': 0.59}\n",
      "{'loss': 1.7455, 'learning_rate': 4.404761904761905e-05, 'epoch': 0.6}\n",
      "{'loss': 1.6998, 'learning_rate': 4.2857142857142856e-05, 'epoch': 0.61}\n",
      "{'loss': 1.6311, 'learning_rate': 4.166666666666667e-05, 'epoch': 0.62}\n",
      "{'loss': 1.6529, 'learning_rate': 4.047619047619048e-05, 'epoch': 0.63}\n",
      "{'loss': 1.6794, 'learning_rate': 3.928571428571429e-05, 'epoch': 0.64}\n",
      "{'loss': 1.6679, 'learning_rate': 3.809523809523809e-05, 'epoch': 0.65}\n",
      "{'loss': 1.5988, 'learning_rate': 3.690476190476191e-05, 'epoch': 0.66}\n",
      "{'loss': 1.6306, 'learning_rate': 3.571428571428572e-05, 'epoch': 0.67}\n",
      "{'loss': 1.6587, 'learning_rate': 3.4523809523809526e-05, 'epoch': 0.68}\n",
      "{'loss': 1.6594, 'learning_rate': 3.3333333333333335e-05, 'epoch': 0.7}\n",
      "{'loss': 1.6443, 'learning_rate': 3.2142857142857144e-05, 'epoch': 0.71}\n",
      "{'loss': 1.6866, 'learning_rate': 3.095238095238095e-05, 'epoch': 0.72}\n",
      "{'loss': 1.6474, 'learning_rate': 2.9761904761904762e-05, 'epoch': 0.73}\n",
      "{'loss': 1.6495, 'learning_rate': 2.857142857142857e-05, 'epoch': 0.74}\n",
      "{'loss': 1.7268, 'learning_rate': 2.7380952380952383e-05, 'epoch': 0.75}\n",
      "{'loss': 1.6488, 'learning_rate': 2.6190476190476192e-05, 'epoch': 0.76}\n",
      "{'loss': 1.6696, 'learning_rate': 2.5e-05, 'epoch': 0.77}\n",
      "{'loss': 1.613, 'learning_rate': 2.380952380952381e-05, 'epoch': 0.78}\n",
      "{'loss': 1.6544, 'learning_rate': 2.261904761904762e-05, 'epoch': 0.79}\n",
      "{'loss': 1.6332, 'learning_rate': 2.1428571428571428e-05, 'epoch': 0.8}\n",
      "{'loss': 1.6545, 'learning_rate': 2.023809523809524e-05, 'epoch': 0.81}\n",
      "{'loss': 1.5423, 'learning_rate': 1.9047619047619046e-05, 'epoch': 0.83}\n",
      "{'loss': 1.589, 'learning_rate': 1.785714285714286e-05, 'epoch': 0.84}\n",
      "{'loss': 1.6196, 'learning_rate': 1.6666666666666667e-05, 'epoch': 0.85}\n",
      "{'loss': 1.5883, 'learning_rate': 1.5476190476190476e-05, 'epoch': 0.86}\n",
      "{'loss': 1.6747, 'learning_rate': 1.4285714285714285e-05, 'epoch': 0.87}\n",
      "{'loss': 1.6532, 'learning_rate': 1.3095238095238096e-05, 'epoch': 0.88}\n",
      "{'loss': 1.6582, 'learning_rate': 1.1904761904761905e-05, 'epoch': 0.89}\n",
      "{'loss': 1.6189, 'learning_rate': 1.0714285714285714e-05, 'epoch': 0.9}\n",
      "{'loss': 1.6074, 'learning_rate': 9.523809523809523e-06, 'epoch': 0.91}\n",
      "{'loss': 1.618, 'learning_rate': 8.333333333333334e-06, 'epoch': 0.92}\n",
      "{'loss': 1.6733, 'learning_rate': 7.142857142857143e-06, 'epoch': 0.93}\n",
      "{'loss': 1.5747, 'learning_rate': 5.9523809523809525e-06, 'epoch': 0.94}\n",
      "{'loss': 1.6336, 'learning_rate': 4.7619047619047615e-06, 'epoch': 0.96}\n",
      "{'loss': 1.5957, 'learning_rate': 3.5714285714285714e-06, 'epoch': 0.97}\n",
      "{'loss': 1.5581, 'learning_rate': 2.3809523809523808e-06, 'epoch': 0.98}\n",
      "{'loss': 1.5907, 'learning_rate': 1.1904761904761904e-06, 'epoch': 0.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5999, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 844.0132, 'train_samples_per_second': 17.455, 'train_steps_per_second': 1.09, 'train_loss': 1.776882411086041, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=920, training_loss=1.776882411086041, metrics={'train_runtime': 844.0132, 'train_samples_per_second': 17.455, 'train_steps_per_second': 1.09, 'train_loss': 1.776882411086041, 'epoch': 1.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(model=model, args=training_args,\n",
    "                  tokenizer=tokenizer, data_collator=seq2seq_collator,\n",
    "                  train_dataset=dataset_samsum_pt[\"train\"],\n",
    "                  eval_dataset=dataset_samsum_pt[\"validation\"])\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 410/410 [02:40<00:00,  2.55it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>rougeLsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bart_finetuned</th>\n",
       "      <td>0.4375</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                rouge1    rouge2  rougeL  rougeLsum\n",
       "bart_finetuned  0.4375  0.133333    0.25       0.25"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate after finetuning\n",
    "score = evaluate_summaries(\n",
    "    dataset_samsum[\"test\"], rouge_metric, trainer.model, tokenizer,\n",
    "    batch_size=2, column_text=\"dialogue\", column_summary=\"summary\")\n",
    "pd.DataFrame(score, index=[f\"bart_finetuned\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = dataset_samsum[\"test\"][0][\"dialogue\"]\n",
    "reference = dataset_samsum[\"test\"][0][\"summary\"]\n",
    "\n",
    "inputs = tokenizer(sample_text, max_length=1024, truncation=True,\n",
    "                   padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "summaries = model.generate(input_ids=inputs[\"input_ids\"].to(device),\n",
    "                           attention_mask=inputs[\"attention_mask\"].to(\n",
    "    device),\n",
    "    length_penalty=0.8, num_beams=8, max_length=128)\n",
    "\n",
    "decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True,\n",
    "                                      clean_up_tokenization_spaces=True)\n",
    "                     for s in summaries]\n",
    "\n",
    "decoded_summaries = [d.replace(\"<n>\", \" \") for d in decoded_summaries]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Amanda can't find Betty's number.\"]\n"
     ]
    }
   ],
   "source": [
    "print(decoded_summaries)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
