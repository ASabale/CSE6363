{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuggingFace\n",
    "\n",
    "[HuggingFace](https://huggingface.co/) is a library for machine learning that provides pre-trained models and datasets. It also provides tools for training and evaluating models. It is a great resource for NLP. We will use it to access some helpful tools as well as the IMDB dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    unsupervised: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "imdb_dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# Print the length of the train and test sets\n",
    "print(imdb_dataset)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "train_dataset, valid_dataset = random_split(imdb_dataset[\"train\"], [20000, 5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I rented I AM CURIOUS-YELLOW from my video sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"I Am Curious: Yellow\" is a risible and preten...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If only to avoid making this type of film in t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This film was probably inspired by Godard's Ma...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Oh, brother...after hearing about this ridicul...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  I rented I AM CURIOUS-YELLOW from my video sto...      0\n",
       "1  \"I Am Curious: Yellow\" is a risible and preten...      0\n",
       "2  If only to avoid making this type of film in t...      0\n",
       "3  This film was probably inspired by Godard's Ma...      0\n",
       "4  Oh, brother...after hearing about this ridicul...      0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as numpy\n",
    "import pandas as pd\n",
    "\n",
    "imdb_dataset.set_format(type=\"pandas\")\n",
    "df = imdb_dataset[\"train\"][:]\n",
    "\n",
    "imdb_dataset.set_format(type=\"torch\")\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "Tokenization is the process of breaking a stream of text up into words, phrases, symbols, or other meaningful elements called tokens. Tokens can be individual words, phrases or even whole sentences. The list of tokens becomes input for further processing.\n",
    "\n",
    "## Character Tokenization\n",
    "\n",
    "Character tokenization is the simplest form of tokenization. It breaks the text into individual characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\x08', '\\t', '\\x10', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~', '\\x80', '\\x84', '\\x85', '\\x8d', '\\x8e', '\\x91', '\\x95', '\\x96', '\\x97', '\\x9a', '\\x9e', '\\xa0', '¡', '¢', '£', '¤', '¦', '§', '¨', '«', '\\xad', '®', '°', '³', '´', '·', 'º', '»', '½', '¾', '¿', 'À', 'Á', 'Ã', 'Ä', 'Å', 'È', 'É', 'Ê', 'Õ', 'Ø', 'Ü', 'ß', 'à', 'á', 'â', 'ã', 'ä', 'å', 'æ', 'ç', 'è', 'é', 'ê', 'ë', 'ì', 'í', 'î', 'ï', 'ð', 'ñ', 'ò', 'ó', 'ô', 'ö', 'ø', 'ù', 'ú', 'û', 'ü', 'ý', 'ō', '–', '‘', '’', '“', '”', '…', '₤', '\\uf0b7']\n",
      "I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it's not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn't have much of a plot.\n",
      "[44, 3, 85, 72, 81, 87, 72, 71, 3, 44, 3, 36, 48, 3, 38, 56, 53, 44, 50, 56, 54, 16, 60, 40, 47, 47, 50, 58, 3, 73, 85, 82, 80, 3, 80, 92, 3, 89, 76, 71, 72, 82, 3, 86, 87, 82, 85, 72, 3, 69, 72, 70, 68, 88, 86, 72, 3, 82, 73, 3, 68, 79, 79, 3, 87, 75, 72, 3, 70, 82, 81, 87, 85, 82, 89, 72, 85, 86, 92, 3, 87, 75, 68, 87, 3, 86, 88, 85, 85, 82, 88, 81, 71, 72, 71, 3, 76, 87, 3, 90, 75, 72, 81, 3, 76, 87, 3, 90, 68, 86, 3, 73, 76, 85, 86, 87, 3, 85, 72, 79, 72, 68, 86, 72, 71, 3, 76, 81, 3, 20, 28, 25, 26, 17, 3, 44, 3, 68, 79, 86, 82, 3, 75, 72, 68, 85, 71, 3, 87, 75, 68, 87, 3, 68, 87, 3, 73, 76, 85, 86, 87, 3, 76, 87, 3, 90, 68, 86, 3, 86, 72, 76, 93, 72, 71, 3, 69, 92, 3, 56, 17, 54, 17, 3, 70, 88, 86, 87, 82, 80, 86, 3, 76, 73, 3, 76, 87, 3, 72, 89, 72, 85, 3, 87, 85, 76, 72, 71, 3, 87, 82, 3, 72, 81, 87, 72, 85, 3, 87, 75, 76, 86, 3, 70, 82, 88, 81, 87, 85, 92, 15, 3, 87, 75, 72, 85, 72, 73, 82, 85, 72, 3, 69, 72, 76, 81, 74, 3, 68, 3, 73, 68, 81, 3, 82, 73, 3, 73, 76, 79, 80, 86, 3, 70, 82, 81, 86, 76, 71, 72, 85, 72, 71, 3, 5, 70, 82, 81, 87, 85, 82, 89, 72, 85, 86, 76, 68, 79, 5, 3, 44, 3, 85, 72, 68, 79, 79, 92, 3, 75, 68, 71, 3, 87, 82, 3, 86, 72, 72, 3, 87, 75, 76, 86, 3, 73, 82, 85, 3, 80, 92, 86, 72, 79, 73, 17, 31, 69, 85, 3, 18, 33, 31, 69, 85, 3, 18, 33, 55, 75, 72, 3, 83, 79, 82, 87, 3, 76, 86, 3, 70, 72, 81, 87, 72, 85, 72, 71, 3, 68, 85, 82, 88, 81, 71, 3, 68, 3, 92, 82, 88, 81, 74, 3, 54, 90, 72, 71, 76, 86, 75, 3, 71, 85, 68, 80, 68, 3, 86, 87, 88, 71, 72, 81, 87, 3, 81, 68, 80, 72, 71, 3, 47, 72, 81, 68, 3, 90, 75, 82, 3, 90, 68, 81, 87, 86, 3, 87, 82, 3, 79, 72, 68, 85, 81, 3, 72, 89, 72, 85, 92, 87, 75, 76, 81, 74, 3, 86, 75, 72, 3, 70, 68, 81, 3, 68, 69, 82, 88, 87, 3, 79, 76, 73, 72, 17, 3, 44, 81, 3, 83, 68, 85, 87, 76, 70, 88, 79, 68, 85, 3, 86, 75, 72, 3, 90, 68, 81, 87, 86, 3, 87, 82, 3, 73, 82, 70, 88, 86, 3, 75, 72, 85, 3, 68, 87, 87, 72, 81, 87, 76, 82, 81, 86, 3, 87, 82, 3, 80, 68, 78, 76, 81, 74, 3, 86, 82, 80, 72, 3, 86, 82, 85, 87, 3, 82, 73, 3, 71, 82, 70, 88, 80, 72, 81, 87, 68, 85, 92, 3, 82, 81, 3, 90, 75, 68, 87, 3, 87, 75, 72, 3, 68, 89, 72, 85, 68, 74, 72, 3, 54, 90, 72, 71, 72, 3, 87, 75, 82, 88, 74, 75, 87, 3, 68, 69, 82, 88, 87, 3, 70, 72, 85, 87, 68, 76, 81, 3, 83, 82, 79, 76, 87, 76, 70, 68, 79, 3, 76, 86, 86, 88, 72, 86, 3, 86, 88, 70, 75, 3, 68, 86, 3, 87, 75, 72, 3, 57, 76, 72, 87, 81, 68, 80, 3, 58, 68, 85, 3, 68, 81, 71, 3, 85, 68, 70, 72, 3, 76, 86, 86, 88, 72, 86, 3, 76, 81, 3, 87, 75, 72, 3, 56, 81, 76, 87, 72, 71, 3, 54, 87, 68, 87, 72, 86, 17, 3, 44, 81, 3, 69, 72, 87, 90, 72, 72, 81, 3, 68, 86, 78, 76, 81, 74, 3, 83, 82, 79, 76, 87, 76, 70, 76, 68, 81, 86, 3, 68, 81, 71, 3, 82, 85, 71, 76, 81, 68, 85, 92, 3, 71, 72, 81, 76, 93, 72, 81, 86, 3, 82, 73, 3, 54, 87, 82, 70, 78, 75, 82, 79, 80, 3, 68, 69, 82, 88, 87, 3, 87, 75, 72, 76, 85, 3, 82, 83, 76, 81, 76, 82, 81, 86, 3, 82, 81, 3, 83, 82, 79, 76, 87, 76, 70, 86, 15, 3, 86, 75, 72, 3, 75, 68, 86, 3, 86, 72, 91, 3, 90, 76, 87, 75, 3, 75, 72, 85, 3, 71, 85, 68, 80, 68, 3, 87, 72, 68, 70, 75, 72, 85, 15, 3, 70, 79, 68, 86, 86, 80, 68, 87, 72, 86, 15, 3, 68, 81, 71, 3, 80, 68, 85, 85, 76, 72, 71, 3, 80, 72, 81, 17, 31, 69, 85, 3, 18, 33, 31, 69, 85, 3, 18, 33, 58, 75, 68, 87, 3, 78, 76, 79, 79, 86, 3, 80, 72, 3, 68, 69, 82, 88, 87, 3, 44, 3, 36, 48, 3, 38, 56, 53, 44, 50, 56, 54, 16, 60, 40, 47, 47, 50, 58, 3, 76, 86, 3, 87, 75, 68, 87, 3, 23, 19, 3, 92, 72, 68, 85, 86, 3, 68, 74, 82, 15, 3, 87, 75, 76, 86, 3, 90, 68, 86, 3, 70, 82, 81, 86, 76, 71, 72, 85, 72, 71, 3, 83, 82, 85, 81, 82, 74, 85, 68, 83, 75, 76, 70, 17, 3, 53, 72, 68, 79, 79, 92, 15, 3, 87, 75, 72, 3, 86, 72, 91, 3, 68, 81, 71, 3, 81, 88, 71, 76, 87, 92, 3, 86, 70, 72, 81, 72, 86, 3, 68, 85, 72, 3, 73, 72, 90, 3, 68, 81, 71, 3, 73, 68, 85, 3, 69, 72, 87, 90, 72, 72, 81, 15, 3, 72, 89, 72, 81, 3, 87, 75, 72, 81, 3, 76, 87, 10, 86, 3, 81, 82, 87, 3, 86, 75, 82, 87, 3, 79, 76, 78, 72, 3, 86, 82, 80, 72, 3, 70, 75, 72, 68, 83, 79, 92, 3, 80, 68, 71, 72, 3, 83, 82, 85, 81, 82, 17, 3, 58, 75, 76, 79, 72, 3, 80, 92, 3, 70, 82, 88, 81, 87, 85, 92, 80, 72, 81, 3, 80, 76, 81, 71, 3, 73, 76, 81, 71, 3, 76, 87, 3, 86, 75, 82, 70, 78, 76, 81, 74, 15, 3, 76, 81, 3, 85, 72, 68, 79, 76, 87, 92, 3, 86, 72, 91, 3, 68, 81, 71, 3, 81, 88, 71, 76, 87, 92, 3, 68, 85, 72, 3, 68, 3, 80, 68, 77, 82, 85, 3, 86, 87, 68, 83, 79, 72, 3, 76, 81, 3, 54, 90, 72, 71, 76, 86, 75, 3, 70, 76, 81, 72, 80, 68, 17, 3, 40, 89, 72, 81, 3, 44, 81, 74, 80, 68, 85, 3, 37, 72, 85, 74, 80, 68, 81, 15, 3, 68, 85, 74, 88, 68, 69, 79, 92, 3, 87, 75, 72, 76, 85, 3, 68, 81, 86, 90, 72, 85, 3, 87, 82, 3, 74, 82, 82, 71, 3, 82, 79, 71, 3, 69, 82, 92, 3, 45, 82, 75, 81, 3, 41, 82, 85, 71, 15, 3, 75, 68, 71, 3, 86, 72, 91, 3, 86, 70, 72, 81, 72, 86, 3, 76, 81, 3, 75, 76, 86, 3, 73, 76, 79, 80, 86, 17, 31, 69, 85, 3, 18, 33, 31, 69, 85, 3, 18, 33, 44, 3, 71, 82, 3, 70, 82, 80, 80, 72, 81, 71, 3, 87, 75, 72, 3, 73, 76, 79, 80, 80, 68, 78, 72, 85, 86, 3, 73, 82, 85, 3, 87, 75, 72, 3, 73, 68, 70, 87, 3, 87, 75, 68, 87, 3, 68, 81, 92, 3, 86, 72, 91, 3, 86, 75, 82, 90, 81, 3, 76, 81, 3, 87, 75, 72, 3, 73, 76, 79, 80, 3, 76, 86, 3, 86, 75, 82, 90, 81, 3, 73, 82, 85, 3, 68, 85, 87, 76, 86, 87, 76, 70, 3, 83, 88, 85, 83, 82, 86, 72, 86, 3, 85, 68, 87, 75, 72, 85, 3, 87, 75, 68, 81, 3, 77, 88, 86, 87, 3, 87, 82, 3, 86, 75, 82, 70, 78, 3, 83, 72, 82, 83, 79, 72, 3, 68, 81, 71, 3, 80, 68, 78, 72, 3, 80, 82, 81, 72, 92, 3, 87, 82, 3, 69, 72, 3, 86, 75, 82, 90, 81, 3, 76, 81, 3, 83, 82, 85, 81, 82, 74, 85, 68, 83, 75, 76, 70, 3, 87, 75, 72, 68, 87, 72, 85, 86, 3, 76, 81, 3, 36, 80, 72, 85, 76, 70, 68, 17, 3, 44, 3, 36, 48, 3, 38, 56, 53, 44, 50, 56, 54, 16, 60, 40, 47, 47, 50, 58, 3, 76, 86, 3, 68, 3, 74, 82, 82, 71, 3, 73, 76, 79, 80, 3, 73, 82, 85, 3, 68, 81, 92, 82, 81, 72, 3, 90, 68, 81, 87, 76, 81, 74, 3, 87, 82, 3, 86, 87, 88, 71, 92, 3, 87, 75, 72, 3, 80, 72, 68, 87, 3, 68, 81, 71, 3, 83, 82, 87, 68, 87, 82, 72, 86, 3, 11, 81, 82, 3, 83, 88, 81, 3, 76, 81, 87, 72, 81, 71, 72, 71, 12, 3, 82, 73, 3, 54, 90, 72, 71, 76, 86, 75, 3, 70, 76, 81, 72, 80, 68, 17, 3, 37, 88, 87, 3, 85, 72, 68, 79, 79, 92, 15, 3, 87, 75, 76, 86, 3, 73, 76, 79, 80, 3, 71, 82, 72, 86, 81, 10, 87, 3, 75, 68, 89, 72, 3, 80, 88, 70, 75, 3, 82, 73, 3, 68, 3, 83, 79, 82, 87, 17]\n",
      "I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it's not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn't have much of a plot.\n"
     ]
    }
   ],
   "source": [
    "# Get unique characters that exist in the dataset\n",
    "unique_chars = set(' '.join(df[:]['text'].tolist()))\n",
    "\n",
    "# Sort the unique characters\n",
    "unique_chars = sorted(list(unique_chars))\n",
    "print(unique_chars)\n",
    "\n",
    "# Tokenize the first input\n",
    "input_text = df[:]['text'].tolist()[0]\n",
    "print(input_text)\n",
    "\n",
    "# Create a dictionary that maps unique characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(unique_chars)}\n",
    "idx2char = {i:u for i, u in enumerate(unique_chars)}\n",
    "\n",
    "# Create encoder and decoder functions\n",
    "encode = lambda s: [char2idx[c] for c in s] # String to list of indices\n",
    "decode = lambda s: ''.join([idx2char[c] for c in s]) # List of indices to string\n",
    "input_seq = encode(input_text)\n",
    "\n",
    "# Print the result of encoding and decoding the first input\n",
    "print(input_seq)\n",
    "print(decode(encode(input_text)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word-level Tokenization\n",
    "\n",
    "Word tokenization is the process of breaking a stream of text up into words. The list of tokens becomes input for further processing. Using words instead of characters means that our model doesn't have to learn the word itself.\n",
    "\n",
    "As opposed to character-level tokenization, word tokenization requires a vastly larger dictionary size. This is because the number of unique words in a corpus is much larger than the number of unique characters. In practice, a subset of the most common words is used to build the dictionary.\n",
    "\n",
    "## Word Tokenization with `PyTorch` and `torchtext`\n",
    "\n",
    "HuggingFace provides access to many popular datasets for NLP. The `torchtext` library provides a simple API for loading and processing text data. It includes a variety of datasets, tokenizers, and data iterators. Using these libraries, we will prepare a dataset as follows:\n",
    "\n",
    "2. Create a vocabulary using the `torchtext` library.\n",
    "3. Convert the text to a sequence of integers using the vocabulary.\n",
    "\n",
    "We will start off by creating our own custom tokenizer. However, there are many other tokenizers available in the `torchtext` library. For example, the `spacy` tokenizer is a popular choice. The `spacy` tokenizer is a rule-based tokenizer that uses the `spaCy` library to tokenize text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  69241\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'<unk>'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/Users/dillhoff/dev/teaching/CSE6363-1/natural_language_processing/text_preprocessing.ipynb Cell 7\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dillhoff/dev/teaching/CSE6363-1/natural_language_processing/text_preprocessing.ipynb#W6sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m sorted_tokens \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m(counter\u001b[39m.\u001b[39mitems(), key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: x[\u001b[39m1\u001b[39m], reverse\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dillhoff/dev/teaching/CSE6363-1/natural_language_processing/text_preprocessing.ipynb#W6sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m ordered_dict \u001b[39m=\u001b[39m OrderedDict(sorted_tokens)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/dillhoff/dev/teaching/CSE6363-1/natural_language_processing/text_preprocessing.ipynb#W6sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m vocab \u001b[39m=\u001b[39m Vocab(ordered_dict)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dillhoff/dev/teaching/CSE6363-1/natural_language_processing/text_preprocessing.ipynb#W6sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39m# Insert <unk> and <pad> tokens for unknown items and padding when batching fixed sized sequences\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dillhoff/dev/teaching/CSE6363-1/natural_language_processing/text_preprocessing.ipynb#W6sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m vocab\u001b[39m.\u001b[39minsert_token(\u001b[39m'\u001b[39m\u001b[39m<pad>\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cse6363/lib/python3.10/site-packages/torchtext/vocab.py:74\u001b[0m, in \u001b[0;36mVocab.__init__\u001b[0;34m(self, counter, max_size, min_freq, specials, vectors, unk_init, vectors_cache, specials_first)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[39m# frequencies of special tokens are not counted when building vocabulary\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[39m# in frequency order\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[39mfor\u001b[39;00m tok \u001b[39min\u001b[39;00m specials:\n\u001b[0;32m---> 74\u001b[0m     \u001b[39mdel\u001b[39;00m counter[tok]\n\u001b[1;32m     76\u001b[0m \u001b[39m# sort by frequency, then alphabetically\u001b[39;00m\n\u001b[1;32m     77\u001b[0m words_and_frequencies \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m(counter\u001b[39m.\u001b[39mitems(), key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m tup: tup[\u001b[39m0\u001b[39m])\n",
      "\u001b[0;31mKeyError\u001b[0m: '<unk>'"
     ]
    }
   ],
   "source": [
    "# Create a tokenizer which removes punctuation and special characters before splitting the text into words.\n",
    "# This tokenizer is from Sebastian Raschka's book \"Machine Learning with PyTorch and sci-kit learn\"\n",
    "import re\n",
    "from collections import Counter, OrderedDict\n",
    "\n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', '')\n",
    "    tokenized = text.split()\n",
    "\n",
    "    return tokenized\n",
    "\n",
    "# Create a counter object to count the number of times each word appears in the dataset\n",
    "counter = Counter()\n",
    "\n",
    "# Loop through each review and tokenize it\n",
    "for sample in train_dataset:\n",
    "    line = sample['text']\n",
    "    counter.update(tokenizer(line))\n",
    "\n",
    "print(\"Vocabulary size: \", len(counter))\n",
    "\n",
    "# Create an encoder\n",
    "from torchtext.vocab import Vocab\n",
    "\n",
    "sorted_tokens = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
    "ordered_dict = OrderedDict(sorted_tokens)\n",
    "vocab = vocab(ordered_dict)\n",
    "\n",
    "# Insert <unk> and <pad> tokens for unknown items and padding when batching fixed sized sequences\n",
    "vocab.insert_token('<pad>', 0)\n",
    "vocab.insert_token('<unk>', 1)\n",
    "vocab.set_default_index(1)\n",
    "\n",
    "# Create a function to encode the text\n",
    "def encode_text(text):\n",
    "    return vocab.lookup_indices(tokenizer(text))\n",
    "\n",
    "# Create a function to decode the text\n",
    "def decode_text(encoded_text):\n",
    "    return vocab.lookup_tokens(encoded_text)\n",
    "\n",
    "# Create a function to encode the labels\n",
    "def encode_label(label):\n",
    "    return 1 if label == 'pos' else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample a random review and tokenize it\n",
    "sample_idx = torch.randint(len(train_dataset), size=(1,)).item()\n",
    "sample_review = train_dataset[sample_idx]['text']\n",
    "sample_label = train_dataset[sample_idx]['label']\n",
    "print(\"Sample review: \", sample_review)\n",
    "\n",
    "# Encode the review\n",
    "encoded_review = encode_text(sample_review)\n",
    "print(\"Encoded review: \", encoded_review)\n",
    "\n",
    "# Decode the review\n",
    "decoded_review = decode_text(encoded_review)\n",
    "print(\"Decoded review: \", decoded_review)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subword Tokenization\n",
    "\n",
    "Subword tokenizations combine the benefits of both character and word-level tokenization. These tokenizers are learned from the data. They are able to break down words into smaller parts, resulting in a smaller vocabulary size. It also allows the model to learn from words that it has not seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# Encode the review\n",
    "encoded_review = enc.encode(sample_review)\n",
    "print(\"Encoded review: \", encoded_review)\n",
    "\n",
    "# Decode the review\n",
    "decoded_review = enc.decode(encoded_review)\n",
    "print(\"Decoded review: \", decoded_review)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopwords\n",
    "\n",
    "Stop words are words that are most common words in a language. For many NLP tasks, these words are not useful and can be removed from the text. NLTK has a list of stopwords for many languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# Download the stopwords from NLTK\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Import the stopword list\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Print the first 10 stopwords\n",
    "stopwords.words('english')[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings\n",
    "\n",
    "Embeddings are a way to represent words as vectors learned from the data. They are able to capture the meaning of words and their relationships to other words. Embeddings are used in many NLP tasks such as machine translation, sentiment analysis, and text classification. Besides providing a way to convert our tokenized input to a lower dimensional space, these embeddings allow words with similar meanings to be close together in the embedding space.\n",
    "\n",
    "Google's Word2Vec is a popular embedding model. It is trained on a large corpus of text. The model learns to predict the context of a word based on its neighbors. The resulting embeddings are able to capture the meaning of words and their relationships to other words. Other popular embeddings include GloVe and FastText.\n",
    "\n",
    "In the context of Transformer-based models, the embeddings are learned as part of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/dillhoff/dev/teaching/CSE6363-1/natural_language_processing/text_preprocessing.ipynb Cell 14\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/dillhoff/dev/teaching/CSE6363-1/natural_language_processing/text_preprocessing.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Sample snippet from https://vaclavkosar.com/ml/transformer-embeddings-and-tokenization\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/dillhoff/dev/teaching/CSE6363-1/natural_language_processing/text_preprocessing.ipynb#X21sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m DistilBertTokenizerFast, DistilBertModel\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/dillhoff/dev/teaching/CSE6363-1/natural_language_processing/text_preprocessing.ipynb#X21sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m DistilBertTokenizerFast\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mdistilbert-base-uncased\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/dillhoff/dev/teaching/CSE6363-1/natural_language_processing/text_preprocessing.ipynb#X21sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m tokens \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mencode(\u001b[39m'\u001b[39m\u001b[39mThis is a input.\u001b[39m\u001b[39m'\u001b[39m, return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "# Sample snippet from https://vaclavkosar.com/ml/transformer-embeddings-and-tokenization\n",
    "\n",
    "from transformers import DistilBertTokenizerFast, DistilBertModel\n",
    "\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "tokens = tokenizer.encode('This is a input.', return_tensors='pt')\n",
    "print(\"These are tokens!\", tokens)\n",
    "for token in tokens[0]:\n",
    "    print(\"This are decoded tokens!\", tokenizer.decode([token]))\n",
    "\n",
    "model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "print(model.embeddings.word_embeddings(tokens))\n",
    "for e in model.embeddings.word_embeddings(tokens)[0]:\n",
    "    print(\"This is an embedding!\", e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
