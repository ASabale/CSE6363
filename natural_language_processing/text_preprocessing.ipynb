{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuggingFace\n",
    "\n",
    "[HuggingFace](https://huggingface.co/) is a library for machine learning that provides pre-trained models and datasets. It also provides tools for training and evaluating models. It is a great resource for NLP. We will use it to access some helpful tools as well as the IMDB dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "imdb_dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# Print the length of the train and test sets\n",
    "print(imdb_dataset)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "train_dataset, valid_dataset = random_split(imdb_dataset[\"train\"], [20000, 5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as numpy\n",
    "import pandas as pd\n",
    "\n",
    "imdb_dataset.set_format(type=\"pandas\")\n",
    "df = imdb_dataset[\"train\"][:]\n",
    "\n",
    "imdb_dataset.set_format(type=\"torch\")\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "Tokenization is the process of breaking a stream of text up into words, phrases, symbols, or other meaningful elements called tokens. Tokens can be individual words, phrases or even whole sentences. The list of tokens becomes input for further processing.\n",
    "\n",
    "## Character Tokenization\n",
    "\n",
    "Character tokenization is the simplest form of tokenization. It breaks the text into individual characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique characters that exist in the dataset\n",
    "unique_chars = set(' '.join(df[:]['text'].tolist()))\n",
    "\n",
    "# Sort the unique characters\n",
    "unique_chars = sorted(list(unique_chars))\n",
    "print(unique_chars)\n",
    "\n",
    "# Tokenize the first input\n",
    "input_text = df[:]['text'].tolist()[0]\n",
    "print(input_text)\n",
    "\n",
    "# Create a dictionary that maps unique characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(unique_chars)}\n",
    "idx2char = {i:u for i, u in enumerate(unique_chars)}\n",
    "\n",
    "# Create encoder and decoder functions\n",
    "encode = lambda s: [char2idx[c] for c in s] # String to list of indices\n",
    "decode = lambda s: ''.join([idx2char[c] for c in s]) # List of indices to string\n",
    "input_seq = encode(input_text)\n",
    "\n",
    "# Print the result of encoding and decoding the first input\n",
    "print(input_seq)\n",
    "print(decode(encode(input_text)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word-level Tokenization\n",
    "\n",
    "Word tokenization is the process of breaking a stream of text up into words. The list of tokens becomes input for further processing. Using words instead of characters means that our model doesn't have to learn the word itself.\n",
    "\n",
    "As opposed to character-level tokenization, word tokenization requires a vastly larger dictionary size. This is because the number of unique words in a corpus is much larger than the number of unique characters. In practice, a subset of the most common words is used to build the dictionary.\n",
    "\n",
    "## Word Tokenization with `PyTorch` and `torchtext`\n",
    "\n",
    "HuggingFace provides access to many popular datasets for NLP. The `torchtext` library provides a simple API for loading and processing text data. It includes a variety of datasets, tokenizers, and data iterators. Using these libraries, we will prepare a dataset as follows:\n",
    "\n",
    "2. Create a vocabulary using the `torchtext` library.\n",
    "3. Convert the text to a sequence of integers using the vocabulary.\n",
    "\n",
    "We will start off by creating our own custom tokenizer. However, there are many other tokenizers available in the `torchtext` library. For example, the `spacy` tokenizer is a popular choice. The `spacy` tokenizer is a rule-based tokenizer that uses the `spaCy` library to tokenize text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tokenizer which removes punctuation and special characters before splitting the text into words.\n",
    "# This tokenizer is from Sebastian Raschka's book \"Machine Learning with PyTorch and sci-kit learn\"\n",
    "import re\n",
    "from collections import Counter, OrderedDict\n",
    "\n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', '')\n",
    "    tokenized = text.split()\n",
    "\n",
    "    return tokenized\n",
    "\n",
    "# Create a counter object to count the number of times each word appears in the dataset\n",
    "counter = Counter()\n",
    "\n",
    "# Loop through each review and tokenize it\n",
    "for sample in train_dataset:\n",
    "    line = sample['text']\n",
    "    counter.update(tokenizer(line))\n",
    "\n",
    "print(\"Vocabulary size: \", len(counter))\n",
    "\n",
    "# Create an encoder\n",
    "from torchtext.vocab import vocab\n",
    "\n",
    "sorted_tokens = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
    "ordered_dict = OrderedDict(sorted_tokens)\n",
    "vocab = vocab(ordered_dict)\n",
    "\n",
    "# Insert <unk> and <pad> tokens for unknown items and padding when batching fixed sized sequences\n",
    "vocab.insert_token('<pad>', 0)\n",
    "vocab.insert_token('<unk>', 1)\n",
    "vocab.set_default_index(1)\n",
    "\n",
    "# Create a function to encode the text\n",
    "def encode_text(text):\n",
    "    return vocab.lookup_indices(tokenizer(text))\n",
    "\n",
    "# Create a function to decode the text\n",
    "def decode_text(encoded_text):\n",
    "    return vocab.lookup_tokens(encoded_text)\n",
    "\n",
    "# Create a function to encode the labels\n",
    "def encode_label(label):\n",
    "    return 1 if label == 'pos' else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample a random review and tokenize it\n",
    "sample_idx = torch.randint(len(train_dataset), size=(1,)).item()\n",
    "sample_review = train_dataset[sample_idx]['text']\n",
    "sample_label = train_dataset[sample_idx]['label']\n",
    "print(\"Sample review: \", sample_review)\n",
    "\n",
    "# Encode the review\n",
    "encoded_review = encode_text(sample_review)\n",
    "print(\"Encoded review: \", encoded_review)\n",
    "\n",
    "# Decode the review\n",
    "decoded_review = decode_text(encoded_review)\n",
    "print(\"Decoded review: \", decoded_review)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subword Tokenization\n",
    "\n",
    "Subword tokenizations combine the benefits of both character and word-level tokenization. These tokenizers are learned from the data. They are able to break down words into smaller parts, resulting in a smaller vocabulary size. It also allows the model to learn from words that it has not seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# Encode the review\n",
    "encoded_review = enc.encode(sample_review)\n",
    "print(\"Encoded review: \", encoded_review)\n",
    "\n",
    "# Decode the review\n",
    "decoded_review = enc.decode(encoded_review)\n",
    "print(\"Decoded review: \", decoded_review)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopwords\n",
    "\n",
    "Stop words are words that are most common words in a language. For many NLP tasks, these words are not useful and can be removed from the text. NLTK has a list of stopwords for many languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# Download the stopwords from NLTK\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Import the stopword list\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Print the first 10 stopwords\n",
    "stopwords.words('english')[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
