{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the dataset for unsupervised pre-training\n",
    "\n",
    "Models like GPT2 and BERT are pre-trained on large amounts of text data. This is done in an unsupervised manner, meaning that the model is not given any labels. The model is only given the text data and it learns to predict the next word in a sequence. This is done by maximizing the likelihood of the next word given the previous words in the sequence.\n",
    "\n",
    "We will use a smaller dataset for this example from IMDB.\n",
    "\n",
    "# Download the dataset\n",
    "\n",
    "Using HuggingFace's Datasets library, we can download the dataset easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (/home/alex/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d91800b116fe4a8d8c7c350d58626165",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/alex/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-8cf8377e8c7bb09f.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    unsupervised: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Create a PyTorch dataset for IMDB movie reviews using HuggingFace's datasets library\n",
    "# https://huggingface.co/datasets/imdb\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer, DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "\n",
    "# Define the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "# Load the IMDB dataset using HuggingFace's load_dataset\n",
    "imdb_dataset = load_dataset('imdb')\n",
    "\n",
    "print(imdb_dataset)\n",
    "\n",
    "# Concatenate the train and test splits into a single dataset\n",
    "unsupervised_dataset = imdb_dataset['unsupervised']\n",
    "\n",
    "# Define a function to tokenize the text\n",
    "def tokenize_function(example):\n",
    "    t = tokenizer(example['text'])\n",
    "    return t\n",
    "\n",
    "\n",
    "# Apply the tokenization to the dataset\n",
    "tokenized_imdb_dataset = unsupervised_dataset.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all rows into a single list\n",
    "all_text = []\n",
    "for row in tokenized_imdb_dataset:\n",
    "    all_text.extend(row['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, dataset, block_size=128):\n",
    "        self.dataset = dataset\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset) - self.block_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = torch.tensor(self.dataset[idx:idx + self.block_size], dtype=torch.long)\n",
    "        labels = torch.tensor(self.dataset[idx + 1:idx + 1 + self.block_size], dtype=torch.long)\n",
    "        return input_ids, labels\n",
    "\n",
    "# Create the PyTorch dataset\n",
    "imdb_dataset = IMDBDataset(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embedding_dim, max_seq_len):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_seq_len, embedding_dim)\n",
    "        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embedding_dim, 2).float() * (-math.log(10000.0) / embedding_dim))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0), :]\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.query = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.key = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.value = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.out = nn.Linear(embedding_dim, embedding_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        bsz, seq_len, embedding_dim = x.size()\n",
    "        k = self.key(x).view(bsz, seq_len, self.num_heads, embedding_dim // self.num_heads).transpose(1, 2)\n",
    "        q = self.query(x).view(bsz, seq_len, self.num_heads, embedding_dim // self.num_heads).transpose(1, 2)\n",
    "        v = self.value(x).view(bsz, seq_len, self.num_heads, embedding_dim // self.num_heads).transpose(1, 2)\n",
    "        \n",
    "        att_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(embedding_dim // self.num_heads)\n",
    "        att_probs = self.softmax(att_scores)\n",
    "        att_probs = self.dropout(att_probs)\n",
    "        \n",
    "        att_output = torch.matmul(att_probs, v)\n",
    "        att_output = att_output.transpose(1, 2).contiguous().view(bsz, seq_len, embedding_dim)\n",
    "        \n",
    "        return self.out(att_output)\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, embedding_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads, hidden_dim):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = MultiHeadAttention(embedding_dim, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(embedding_dim)\n",
    "        self.feedforward = FeedForward(embedding_dim, hidden_dim)\n",
    "        self.norm2 = nn.LayerNorm(embedding_dim)\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        att_output = self.attention(x)\n",
    "        norm1_output = self.norm1(att_output + x)\n",
    "        feedforward_output = self.feedforward(norm1_output)\n",
    "        output = self.dropout(self.norm2(feedforward_output + norm1_output))\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "class GPT2(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_heads, hidden_dim, num_layers):\n",
    "        super(GPT2, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.positional_encoding = PositionalEncoding(embedding_dim, max_seq_len=512)\n",
    "        self.layers = nn.ModuleList([TransformerBlock(embedding_dim, num_heads, hidden_dim) for _ in range(num_layers)])\n",
    "        self.fc = nn.Linear(embedding_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        embedded = self.positional_encoding(embedded)\n",
    "        for layer in self.layers:\n",
    "            embedded = layer(embedded)\n",
    "        output = self.fc(embedded)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling\n",
    "import pytorch_lightning as pl\n",
    "import torchmetrics\n",
    "\n",
    "# define model hyperparameters\n",
    "vocab_size = 50257 + 1 # Add 1 for the padding token\n",
    "embedding_dim = 256\n",
    "num_heads = 8\n",
    "hidden_dim = 3072\n",
    "num_layers = 4\n",
    "\n",
    "# create data loader and data collator\n",
    "data_loader = DataLoader(imdb_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "class GPT2LightningModule(pl.LightningModule):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_heads, hidden_dim, num_layers):\n",
    "        super(GPT2LightningModule, self).__init__()\n",
    "        self.model = GPT2(vocab_size, embedding_dim, num_heads, hidden_dim, num_layers)\n",
    "        self.perp = torchmetrics.Perplexity()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def generate(self, x, max_len=512):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            generated = x\n",
    "            for _ in range(max_len):\n",
    "                outputs = self.model(generated)\n",
    "                logits = outputs[:, -1, :]\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                next_token = torch.multinomial(probs, num_samples=1)\n",
    "                # next_token = torch.argmax(outputs[:, -1, :], keepdim=True, dim=-1)\n",
    "                generated = torch.cat((generated, next_token), dim=1)\n",
    "        return generated\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        outputs = self.model(inputs)\n",
    "        loss = F.cross_entropy(outputs.view(-1, vocab_size), labels.view(-1))\n",
    "        self.perp(outputs, labels)\n",
    "        self.log('train_loss', loss)\n",
    "        self.log('train_perplexity', self.perp, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type       | Params\n",
      "-------------------------------------\n",
      "0 | model | GPT2       | 33.1 M\n",
      "1 | perp  | Perplexity | 0     \n",
      "-------------------------------------\n",
      "33.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "33.1 M    Total params\n",
      "132.576   Total estimated model params size (MB)\n",
      "/home/alex/anaconda3/envs/pytorch2/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2924a711765470da6ff0a7160b4e6b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=1000` reached.\n"
     ]
    }
   ],
   "source": [
    "# Create a trainer and train the model for 1000 steps\n",
    "trainer = pl.Trainer(accelerator='gpu', max_steps=1000)\n",
    "trainer.fit(GPT2LightningModule(vocab_size, embedding_dim, num_heads, hidden_dim, num_layers), data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 133])\n",
      "the movie was good because bad among good film because they ain't help they made why Romero said up i did because unfortunately i guess i know i feel Cannonies i was good movie was fine movieobaados i did i. Maybe i can't bad /><br /><br />iddies i couldn't been improved the movie didn't know why they does not actually have broken didn't degenerated with obviously i can't help they did did. honestly didn't know when i can't beautiful and i didn't know why i always did i simply didn't ever to do with funny at CHO didn't know i didn't been a TV movie wasn't seem he\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Use the trained model to generate text\n",
    "model = GPT2LightningModule(vocab_size, embedding_dim, num_heads, hidden_dim, num_layers)\n",
    "model.load_state_dict(torch.load('lightning_logs/version_11/checkpoints/epoch=0-step=10000.ckpt')['state_dict'])\n",
    "model.eval()\n",
    "\n",
    "# Encode the prompt\n",
    "prompt = \"the movie was good because\"\n",
    "encoded_prompt = tokenizer.encode(prompt, add_special_tokens=False, return_tensors='pt')\n",
    "\n",
    "# Generate text\n",
    "generated = model.generate(encoded_prompt, max_len=128)\n",
    "print(generated.shape)\n",
    "\n",
    "# Decode the generated text\n",
    "generated_text = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Restoring states from the checkpoint path at lightning_logs/version_10/checkpoints/epoch=0-step=2000.ckpt\n",
      "/home/alex/anaconda3/envs/pytorch2/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:337: UserWarning: The dirpath has changed from '/home/alex/dev/teaching/CSE6363/natural_language_processing/lightning_logs/version_10/checkpoints' to '/home/alex/dev/teaching/CSE6363/natural_language_processing/lightning_logs/version_11/checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded.\n",
      "  warnings.warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type       | Params\n",
      "-------------------------------------\n",
      "0 | model | GPT2       | 33.1 M\n",
      "1 | perp  | Perplexity | 0     \n",
      "-------------------------------------\n",
      "33.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "33.1 M    Total params\n",
      "132.576   Total estimated model params size (MB)\n",
      "Restored all states from the checkpoint at lightning_logs/version_10/checkpoints/epoch=0-step=2000.ckpt\n",
      "/home/alex/anaconda3/envs/pytorch2/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c8fd1d8a5fe44fc86224349a1a7f0e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/anaconda3/envs/pytorch2/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py:151: UserWarning: You're resuming from a checkpoint that ended before the epoch ended. This can cause unreliable results if further training is done. Consider using an end-of-epoch checkpoint\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_steps=10000` reached.\n"
     ]
    }
   ],
   "source": [
    "# Continue training the model\n",
    "trainer = pl.Trainer(accelerator='gpu', max_steps=10000)\n",
    "trainer.fit(model, data_loader, ckpt_path=\"lightning_logs/version_10/checkpoints/epoch=0-step=2000.ckpt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
