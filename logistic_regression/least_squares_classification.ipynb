{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bda774fd-eb24-4e0b-9ef1-48776d8695e9",
   "metadata": {},
   "source": [
    "# Classification with Least Squares\n",
    "\n",
    "This notebook demonstrates how to implement a $K$-class classifier and solve for the parameters using a least-squares approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34ca19f-a43f-47c8-81f7-3e0e51bf2dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from mlxtend.plotting.decision_regions import plot_decision_regions # https://rasbt.github.io/mlxtend/\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec92d49-375a-425b-a04d-db5a071ac3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_hot(targets, nb_classes):\n",
    "    res = np.eye(nb_classes)[np.array(targets).reshape(-1)]\n",
    "    return res.reshape(list(targets.shape)+[nb_classes])\n",
    "\n",
    "\n",
    "class LinearDiscriminant:        \n",
    "    def fit(self, data, targets):\n",
    "        num_classes = np.max(targets, axis=0) + 1\n",
    "        data = np.concatenate((np.ones((data.shape[0], 1)), data), axis=-1)\n",
    "        targets = get_one_hot(targets, num_classes)\n",
    "        self.weights_ = np.linalg.inv(data.T @ data) @ data.T @ targets\n",
    "        \n",
    "    def predict(self, x):\n",
    "        \"\"\"Classify input sample(s)\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : array-like, [n_samples, n_features]\n",
    "            Samples\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        result : array-like, int, [n_samples]\n",
    "            Corresponding prediction(s)\n",
    "        \"\"\"\n",
    "        # Add constant for bias parameter\n",
    "        x = np.concatenate((np.ones((x.shape[0], 1)), x), axis=-1)\n",
    "\n",
    "        return np.argmax(self.weights_.T @ x.T, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6bcfa7-643d-4f2b-a9a3-f700afd699bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some data\n",
    "n_classes = 3\n",
    "X, Y = make_classification(200, 2, n_redundant=0, n_classes=n_classes, n_clusters_per_class=1)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(X[:, 0], X[:, 1], c=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bda7878-e872-4306-90f7-09cab069eeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LinearDiscriminant()\n",
    "classifier.fit(X, Y)\n",
    "\n",
    "# Measure number of misclassifications\n",
    "error = np.sum(np.abs(classifier.predict(X) - Y))\n",
    "print(f\"Error = {(error / 200) * 100:1.2f}%\")\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plot_decision_regions(X, Y, classifier)\n",
    "fig.add_subplot(ax)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "27460336-6946-427b-bc65-c535ac26f9e1",
   "metadata": {},
   "source": [
    "# Sensitivity to Outliers\n",
    "\n",
    "A major downside to least squares models is their sensitivity to outliers.\n",
    "Consider the dataset below which has a relatively balanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99e9b50-27b4-4b3f-a339-bae3aa63038e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_samples = np.random.multivariate_normal([-1, 1], [[0.2, 0], [0, 0.2]], 100)\n",
    "b_samples = np.random.multivariate_normal([1, -1], [[0.2, 0], [0, 0.2]], 100)\n",
    "a_targets = np.zeros(100).astype(int)  # Samples from class A are assigned a class value of 0.\n",
    "b_targets = np.ones(100).astype(int)  # Samples from class B are assigned a class value of 1.\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(a_samples[:, 0], a_samples[:, 1], c='b')\n",
    "ax.scatter(b_samples[:, 0], b_samples[:, 1], c='r')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f30446a6-aba2-4ef0-b615-0dc69583b53a",
   "metadata": {},
   "source": [
    "The data is clearly linearly separable, so a linear classifier should achieve 100% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8bfd01-9610-40f3-a790-1e6d585dc4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate((a_samples, b_samples))\n",
    "Y = np.concatenate((a_targets, b_targets))\n",
    "\n",
    "classifier = LinearDiscriminant()\n",
    "classifier.fit(X, Y)\n",
    "\n",
    "# Measure number of misclassifications\n",
    "error = np.sum(np.abs(classifier.predict(X) - Y))\n",
    "print(f\"Error = {(error / 200) * 100:1.2f}%\")\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plot_decision_regions(X, Y, classifier)\n",
    "fig.add_subplot(ax)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "19dfce76-1438-4afd-bba1-6623d8a029c7",
   "metadata": {},
   "source": [
    "As expected, this is a perfect dataset for a linear classifier.\n",
    "Let's now look at how moving some of the points away from the central cluster will affect the resulting classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c6e0be-9c5f-4a9a-87d5-900417daf352",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_samples1 = np.random.multivariate_normal([-1, 1], [[0.2, 0], [0.2, 0.2]], 80)\n",
    "a_samples2 = np.random.multivariate_normal([-2, 6], [[0.2, 0], [0, 0.2]], 20)\n",
    "a_samples = np.concatenate((a_samples1, a_samples2))\n",
    "b_samples = np.random.multivariate_normal([1, -1], [[0.2, 0], [0, 0.2]], 100)\n",
    "a_targets = np.zeros(100).astype(int)  # Samples from class A are assigned a class value of 0.\n",
    "b_targets = np.ones(100).astype(int)  # Samples from class B are assigned a class value of 1.\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(a_samples[:, 0], a_samples[:, 1], c='b')\n",
    "ax.scatter(b_samples[:, 0], b_samples[:, 1], c='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a68467f-4845-484e-b110-26537fae5022",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate((a_samples, b_samples))\n",
    "Y = np.concatenate((a_targets, b_targets))\n",
    "\n",
    "classifier = LinearDiscriminant()\n",
    "classifier.fit(X, Y)\n",
    "\n",
    "# Measure number of misclassifications\n",
    "error = np.sum(np.abs(classifier.predict(X) - Y))\n",
    "print(f\"Error = {(error / 200) * 100:1.2f}%\")\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plot_decision_regions(X, Y, classifier)\n",
    "fig.add_subplot(ax)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
