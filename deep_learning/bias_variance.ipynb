{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0b4e318-8dc4-4993-a51a-615dba4831b0",
   "metadata": {},
   "source": [
    "# Bias and Variance\n",
    "\n",
    "There is a balance between the bias, variance, and complexity of a model.\n",
    "A simple model with few parameters may not have sufficient representation to generalize on a large diverse dataset.\n",
    "In that case, the model would exhibit high bias.\n",
    "\n",
    "An overly complex model trained on a simple dataset which have high variance.\n",
    "The result is a model that overfits the training data.\n",
    "\n",
    "This notebook will explore a few scenarios on a simple dataset and introduce a few common solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "289bc8f1-42c6-460d-bc21-0e714e06f32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# import wandb\n",
    "from torchvision import transforms\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# wandb.init(project=\"bias-variance-experiments\", entity=\"ajdillhoff\")\n",
    "\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f5cdd52-329d-43e5-b2fa-eb1bc1294024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# root_path = \"/home/alex/Data/CIFAR10\"\n",
    "root_path = \"data/cifar10\"\n",
    "\n",
    "def show(imgs):\n",
    "    if not isinstance(imgs, list):\n",
    "        imgs = [imgs]\n",
    "    fix, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
    "    for i, img in enumerate(imgs):\n",
    "        img = img.detach()\n",
    "        img = transforms.functional.to_pil_image(img)\n",
    "        axs[0, i].imshow(np.asarray(img))\n",
    "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "        \n",
    "        \n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c5f53c-35f2-4079-90be-13516bf1e88f",
   "metadata": {},
   "source": [
    "# Model Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b36eb031-84f1-4ed5-88f7-d998e6ee732b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer, logger=None):\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    pbar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "    for i, (input, target) in pbar:\n",
    "        \n",
    "        input = input.cuda()\n",
    "        target = target.cuda()\n",
    "\n",
    "        output = model(input)\n",
    "        loss = loss_fn(output, target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        output = output.float()\n",
    "        loss = loss.float()\n",
    "\n",
    "        prec = accuracy(output.data, target)[0]\n",
    "        losses.update(loss.item(), input.shape[0])\n",
    "        top1.update(prec.item(), input.shape[0])\n",
    "        \n",
    "        # wandb.log({\"training loss\": loss})\n",
    "\n",
    "        if i % print_frequency == 0:\n",
    "            pbar.set_description(\"Epoch [%d]\\t Loss %.2f\\t Prec@1 %.3f (%.3f)\" % (epoch, losses.avg, top1.val, top1.avg))\n",
    "            if logger:\n",
    "                logger.add_scalar(\"training loss\",\n",
    "                                  loss.item(),\n",
    "                                  epoch * len(dataloader) + i)\n",
    "           \n",
    "        \n",
    "def val_loop(dataloader, model, loss_fn, logger=None):\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    pbar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "    for i, (input, target) in pbar:\n",
    "        \n",
    "        input = input.cuda()\n",
    "        target = target.cuda()\n",
    "\n",
    "        output = model(input)\n",
    "        loss = loss_fn(output, target)\n",
    "\n",
    "        output = output.float()\n",
    "        loss = loss.float()\n",
    "        \n",
    "        # wandb.log({\"validation loss\": loss})\n",
    "\n",
    "        prec = accuracy(output.data, target)[0]\n",
    "        losses.update(loss.item(), input.shape[0])\n",
    "        top1.update(prec.item(), input.shape[0])\n",
    "\n",
    "        if i % print_frequency == 0:\n",
    "            pbar.set_description(\"Epoch [%d]\\t Loss %.2f\\t Prec@1 %.3f (%.3f)\" % (epoch, losses.avg, top1.val, top1.avg))\n",
    "            if logger:\n",
    "                logger.add_scalar(\"validation loss\",\n",
    "                                  loss.item(),\n",
    "                                  epoch * len(dataloader) + i)\n",
    "    \n",
    "    if logger:\n",
    "        logger.add_scalar(\"validation accuracy\",\n",
    "                          top1.avg,\n",
    "                          epoch)\n",
    "            \n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    pbar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "    for i, (input, target) in pbar:\n",
    "        \n",
    "        input = input.cuda()\n",
    "        target = target.cuda()\n",
    "\n",
    "        output = model(input)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        output = output.float()\n",
    "        loss = loss.float()\n",
    "\n",
    "        prec = accuracy(output.data, target)[0]\n",
    "        losses.update(loss.item(), input.shape[0])\n",
    "        top1.update(prec.item(), input.shape[0])\n",
    "\n",
    "    # Print result\n",
    "    print(f\"Average Loss: {losses.avg:>8f}\\nAccuracy: {top1.avg}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44577ed5-26b8-4472-9c13-483382a28071",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9678764e-ee24-4fe1-bffe-e0cbe9081e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Parameters\n",
    "batch_size = 256\n",
    "learning_rate = 1e-3\n",
    "epochs = 5\n",
    "print_frequency = 100\n",
    "\n",
    "# wandb.config = {\n",
    "#     \"learning_rate\": learning_rate,\n",
    "#     \"epochs\": epochs,\n",
    "#     \"batch_size\": batch_size\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b66789b4-4261-4ceb-80ec-5c71261ba625",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(1024, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 10)\n",
    ")\n",
    "\n",
    "# model = nn.Sequential(\n",
    "#     nn.Linear(1024, 10)\n",
    "# )\n",
    "\n",
    "model.cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460f50ba-6aba-45a3-ba27-1164c9929b12",
   "metadata": {},
   "source": [
    "# Data Loader\n",
    "\n",
    "For batch sampling of our dataset, we wrap the dataset object in a `DataLoader` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72481620-6c2a-44ac-87b6-060cfbbc62ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "dataset = torchvision.datasets.CIFAR10(root_path, train=True, transform=transforms.Compose([                                                                     \n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "    transforms.Grayscale(),\n",
    "    torch.flatten\n",
    "]), download=True)\n",
    "\n",
    "dataset_size = len(dataset)\n",
    "train_size = int(dataset_size * .95)\n",
    "val_size = dataset_size - train_size\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size, shuffle=True,\n",
    "    num_workers=8, pin_memory=False)\n",
    "\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size, shuffle=False,\n",
    "    num_workers=8, pin_memory=False)\n",
    "\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.CIFAR10(root_path, train=False, transform=transforms.Compose([                                                                     \n",
    "        transforms.ToTensor(),\n",
    "        # normalize,\n",
    "        transforms.Grayscale(),\n",
    "        torch.flatten\n",
    "    ])),\n",
    "    batch_size=batch_size, shuffle=False,\n",
    "    num_workers=8, pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9062d2c-55be-4d7a-97c3-c43eae836068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4727d16358f473d8f564ff43d2b5503",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/186 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a9ecc6a2c0f4cff942a78dda92acfb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0128792bbb6b41059e7112b047982db3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/186 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0549d0139ebd470585b4df1a4de2761e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0350d58748a546dbb6c91a6a2e86a4bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/186 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb0eb2e4c82942be92f0d20f934749bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27c1a9e40d5d40b4a11159c6762d3a17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/186 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac7b5c1fe52a42f49ec76aca3030dd8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24ee7ae493634d878aa2f216fd7cd184",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/186 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ea027a87a874a01b4bcd31111177261",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logger = SummaryWriter(\"runs/bias_variance_1\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loop(train_dataloader, model, criterion, optimizer, logger)\n",
    "    val_loop(val_dataloader, model, criterion, logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ddd34f-9c1f-4978-8cd1-ac926cddc6d3",
   "metadata": {},
   "source": [
    "# Evaluate The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "89d2805b-e578-43a2-8ff8-1b2810a63c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W ParallelNative.cpp:229] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
      "[W ParallelNative.cpp:229] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
      "[W ParallelNative.cpp:229] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
      "[W ParallelNative.cpp:229] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
      "[W ParallelNative.cpp:229] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
      "[W ParallelNative.cpp:229] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
      "[W ParallelNative.cpp:229] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
      "[W ParallelNative.cpp:229] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9f37040f24c4fc2ac8fede8881a14dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W ParallelNative.cpp:229] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 1.613248\n",
      "Accuracy: 43.74\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_loop(test_dataloader, model, criterion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('cse6363')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "53ef4221aa1ddcebf8ffe152ce07046cd10cba6b415b9c2173d01d52e94eb207"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
