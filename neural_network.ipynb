{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24a3e817-c112-4d12-8426-49647f96a62f",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41ab5b50-0cf9-48d6-bd47-ef0a74d98e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import fetch_openml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0234cf0-97f5-4ff3-8194-ed5673c35e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples, targets = fetch_openml(\"mnist_784\", version=1, return_X_y=True, as_frame=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "bfa21950-83c7-46e5-a16b-a2fc7fa0df6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "samples = samples / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "b6634f8c-d26f-49a9-a8c1-bec5c09f7987",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_hot(targets, nb_classes):\n",
    "    res = np.eye(nb_classes)[np.array(targets).reshape(-1)]\n",
    "    return res.reshape(list(targets.shape)+[nb_classes])\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1. / (1. + np.exp(-x))\n",
    "\n",
    "\n",
    "def sigmoid_grad(out, grad_in):\n",
    "    return out * (1 - out) * grad_in\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    num = np.exp(x - np.max(x))\n",
    "    \n",
    "    return num / np.sum(num, axis=0, keepdims=True)\n",
    "\n",
    "\n",
    "# def softmax_grad(out, grad_in):\n",
    "#       return (np.einsum('ij,jk->ijk', out, np.eye(out.shape[-1])) \\\n",
    "#            - np.einsum('ij,ik->ijk', out, out))\n",
    "    \n",
    "    \n",
    "def softmax_grad(probs, bp_err):\n",
    "    dim = probs.shape[1]\n",
    "    output = np.empty(probs.shape)\n",
    "    for j in range(dim):\n",
    "        d_prob_over_xj = - (probs * probs[:,[j]])  # i.e. prob_k * prob_j, no matter k==j or not\n",
    "        d_prob_over_xj[:,j] += probs[:,j]   # i.e. when k==j, +prob_j\n",
    "        output[:,j] = np.sum(bp_err * d_prob_over_xj, axis=1)\n",
    "    return output\n",
    "\n",
    "\n",
    "def cross_entropy_loss(pred, target):\n",
    "    return -target * np.log(pred)\n",
    "\n",
    "\n",
    "def cross_entropy_grad(pred, target):\n",
    "    return pred - target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "2601a58a-b102-45d6-a4c8-71bb2769455d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = samples.shape[1]\n",
    "num_classes = 10\n",
    "\n",
    "# Hidden layer configuration\n",
    "num_nodes = 200\n",
    "layer1_weights = (np.random.rand(num_nodes, num_features + 1) - 0.5)\n",
    "\n",
    "# Output layer configuration\n",
    "layer2_weights = (np.random.rand(num_classes, num_nodes + 1) - 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3398543-370e-4054-8c3a-b05b09ee7d04",
   "metadata": {},
   "source": [
    "# Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "b49f7467-41f0-4ba7-9892-4d43b43c87f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1_a = layer1_weights @ np.concatenate((np.ones((samples.shape[0], 1)), samples), axis=1).T\n",
    "layer1_z = sigmoid(layer1_a)\n",
    "layer2_a = layer2_weights @ np.concatenate((np.ones((layer1_a.shape[1], 1)), layer1_a.T), axis=1).T\n",
    "layer2_z = softmax(layer2_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "d52b697b-0302-4435-b37d-363ff8e6b21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = cross_entropy_loss(layer2_z.T, get_one_hot(targets.astype(int), 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "d51f83f4-79cc-4182-86ce-49a037821a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.49095674734101\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(np.mean(loss, axis=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99c7691-c7dc-4532-afbc-994360018520",
   "metadata": {},
   "source": [
    "# Backward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "4c42955a-86b8-43f5-a0e9-0aba47f75679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 201)\n"
     ]
    }
   ],
   "source": [
    "d_loss = cross_entropy_grad(layer2_z.T, get_one_hot(targets.astype(int), 10))\n",
    "d_layer2_z = softmax_grad(layer2_z.T, d_loss)\n",
    "print(layer2_weights.shape)\n",
    "# d_layer2_weights = d_layer2_z * np.concatenate((np.ones((layer1_a.shape[1], 1)), layer1_a.T), axis=1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "acd9aa29-d236-4cd4-a6fa-e7b6eff63bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.75858561e-11 -5.64890638e-15 -1.27426123e-03 -5.86122676e-21\n",
      " -6.17716959e-02 -5.42094108e-05 -1.74975953e-04 -4.26398328e-11\n",
      "  6.32751425e-02 -2.12157850e-12]\n"
     ]
    }
   ],
   "source": [
    "print(d_softmax[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cse6363] *",
   "language": "python",
   "name": "conda-env-cse6363-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
