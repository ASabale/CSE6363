{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Summarization Model\n",
    "\n",
    "Now let's see how we can use `HuggingFace` to train a summarization model on a new dataset. We'll use the SAMSum dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split lengths: [14732, 819, 818]\n",
      "Features: ['id', 'dialogue', 'summary']\n",
      "\n",
      "Dialogue:\n",
      "Hannah: Hey, do you have Betty's number?\n",
      "Amanda: Lemme check\n",
      "Hannah: <file_gif>\n",
      "Amanda: Sorry, can't find it.\n",
      "Amanda: Ask Larry\n",
      "Amanda: He called her last time we were at the park together\n",
      "Hannah: I don't know him well\n",
      "Hannah: <file_gif>\n",
      "Amanda: Don't be shy, he's very nice\n",
      "Hannah: If you say so..\n",
      "Hannah: I'd rather you texted him\n",
      "Amanda: Just text him ðŸ™‚\n",
      "Hannah: Urgh.. Alright\n",
      "Hannah: Bye\n",
      "Amanda: Bye bye\n",
      "\n",
      "Summary\n",
      "Hannah needs Betty's number but Amanda doesn't have it. She needs to contact Larry.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "dataset_samsum = load_dataset(\"samsum\")\n",
    "split_lengths = [len(dataset_samsum[split]) for split in dataset_samsum]\n",
    "\n",
    "print(f\"Split lengths: {split_lengths}\")\n",
    "print(f\"Features: {dataset_samsum['train'].column_names}\")\n",
    "print(f\"\\nDialogue:\")\n",
    "print(dataset_samsum[\"test\"][0][\"dialogue\"])\n",
    "print(\"\\nSummary\")\n",
    "print(dataset_samsum[\"test\"][0][\"summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Couldn't build proto file into descriptor pool: duplicate file name sentencepiece_model.proto",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m pipeline\n\u001b[1;32m      3\u001b[0m \u001b[39m# Evaluate this using PEGASUS\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m pipe \u001b[39m=\u001b[39m pipeline(\u001b[39m\"\u001b[39;49m\u001b[39msummarization\u001b[39;49m\u001b[39m\"\u001b[39;49m, model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mgoogle/pegasus-cnn_dailymail\u001b[39;49m\u001b[39m\"\u001b[39;49m, framework\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      5\u001b[0m pipe_out \u001b[39m=\u001b[39m pipe(dataset_samsum[\u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mdialogue\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m      6\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mSummary:\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/cse6363/lib/python3.11/site-packages/transformers/pipelines/__init__.py:885\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, use_auth_token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    882\u001b[0m             tokenizer_kwargs \u001b[39m=\u001b[39m model_kwargs\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m    883\u001b[0m             tokenizer_kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mtorch_dtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> 885\u001b[0m         tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    886\u001b[0m             tokenizer_identifier, use_fast\u001b[39m=\u001b[39;49muse_fast, _from_pipeline\u001b[39m=\u001b[39;49mtask, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtokenizer_kwargs\n\u001b[1;32m    887\u001b[0m         )\n\u001b[1;32m    889\u001b[0m \u001b[39mif\u001b[39;00m load_image_processor:\n\u001b[1;32m    890\u001b[0m     \u001b[39m# Try to infer image processor from model or config name (if provided as str)\u001b[39;00m\n\u001b[1;32m    891\u001b[0m     \u001b[39mif\u001b[39;00m image_processor \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/cse6363/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:711\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    709\u001b[0m tokenizer_class_py, tokenizer_class_fast \u001b[39m=\u001b[39m TOKENIZER_MAPPING[\u001b[39mtype\u001b[39m(config)]\n\u001b[1;32m    710\u001b[0m \u001b[39mif\u001b[39;00m tokenizer_class_fast \u001b[39mand\u001b[39;00m (use_fast \u001b[39mor\u001b[39;00m tokenizer_class_py \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 711\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer_class_fast\u001b[39m.\u001b[39;49mfrom_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49minputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    712\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    713\u001b[0m     \u001b[39mif\u001b[39;00m tokenizer_class_py \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/cse6363/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1812\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1809\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1810\u001b[0m         logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mloading file \u001b[39m\u001b[39m{\u001b[39;00mfile_path\u001b[39m}\u001b[39;00m\u001b[39m from cache at \u001b[39m\u001b[39m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1812\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_from_pretrained(\n\u001b[1;32m   1813\u001b[0m     resolved_vocab_files,\n\u001b[1;32m   1814\u001b[0m     pretrained_model_name_or_path,\n\u001b[1;32m   1815\u001b[0m     init_configuration,\n\u001b[1;32m   1816\u001b[0m     \u001b[39m*\u001b[39;49minit_inputs,\n\u001b[1;32m   1817\u001b[0m     use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m   1818\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   1819\u001b[0m     local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m   1820\u001b[0m     _commit_hash\u001b[39m=\u001b[39;49mcommit_hash,\n\u001b[1;32m   1821\u001b[0m     _is_local\u001b[39m=\u001b[39;49mis_local,\n\u001b[1;32m   1822\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   1823\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/cse6363/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1975\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, use_auth_token, cache_dir, local_files_only, _commit_hash, _is_local, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1973\u001b[0m \u001b[39m# Instantiate tokenizer.\u001b[39;00m\n\u001b[1;32m   1974\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1975\u001b[0m     tokenizer \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(\u001b[39m*\u001b[39;49minit_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minit_kwargs)\n\u001b[1;32m   1976\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[1;32m   1977\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\n\u001b[1;32m   1978\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUnable to load vocabulary from file. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1979\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease check that the provided vocabulary is accessible and not corrupted.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1980\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/cse6363/lib/python3.11/site-packages/transformers/models/pegasus/tokenization_pegasus_fast.py:142\u001b[0m, in \u001b[0;36mPegasusTokenizerFast.__init__\u001b[0;34m(self, vocab_file, tokenizer_file, pad_token, eos_token, unk_token, mask_token, mask_token_sent, additional_special_tokens, offset, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m     additional_special_tokens \u001b[39m=\u001b[39m [mask_token_sent] \u001b[39mif\u001b[39;00m mask_token_sent \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m []\n\u001b[1;32m    140\u001b[0m     additional_special_tokens \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m<unk_\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m>\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m2\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moffset)]\n\u001b[0;32m--> 142\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m    143\u001b[0m     vocab_file,\n\u001b[1;32m    144\u001b[0m     tokenizer_file\u001b[39m=\u001b[39;49mtokenizer_file,\n\u001b[1;32m    145\u001b[0m     pad_token\u001b[39m=\u001b[39;49mpad_token,\n\u001b[1;32m    146\u001b[0m     eos_token\u001b[39m=\u001b[39;49meos_token,\n\u001b[1;32m    147\u001b[0m     unk_token\u001b[39m=\u001b[39;49munk_token,\n\u001b[1;32m    148\u001b[0m     mask_token\u001b[39m=\u001b[39;49mmask_token,\n\u001b[1;32m    149\u001b[0m     mask_token_sent\u001b[39m=\u001b[39;49mmask_token_sent,\n\u001b[1;32m    150\u001b[0m     offset\u001b[39m=\u001b[39;49moffset,\n\u001b[1;32m    151\u001b[0m     additional_special_tokens\u001b[39m=\u001b[39;49madditional_special_tokens,\n\u001b[1;32m    152\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    153\u001b[0m )\n\u001b[1;32m    154\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab_file \u001b[39m=\u001b[39m vocab_file\n\u001b[1;32m    155\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcan_save_slow_tokenizer \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab_file \u001b[39melse\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/cse6363/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py:114\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m     fast_tokenizer \u001b[39m=\u001b[39m TokenizerFast\u001b[39m.\u001b[39mfrom_file(fast_tokenizer_file)\n\u001b[1;32m    112\u001b[0m \u001b[39melif\u001b[39;00m slow_tokenizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    113\u001b[0m     \u001b[39m# We need to convert a slow tokenizer to build the backend\u001b[39;00m\n\u001b[0;32m--> 114\u001b[0m     fast_tokenizer \u001b[39m=\u001b[39m convert_slow_tokenizer(slow_tokenizer)\n\u001b[1;32m    115\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mslow_tokenizer_class \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m     \u001b[39m# We need to create and convert a slow tokenizer to build the backend\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     slow_tokenizer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mslow_tokenizer_class(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/cse6363/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py:1303\u001b[0m, in \u001b[0;36mconvert_slow_tokenizer\u001b[0;34m(transformer_tokenizer)\u001b[0m\n\u001b[1;32m   1295\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1296\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAn instance of tokenizer class \u001b[39m\u001b[39m{\u001b[39;00mtokenizer_class_name\u001b[39m}\u001b[39;00m\u001b[39m cannot be converted in a Fast tokenizer instance.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1297\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m No converter was found. Currently available slow->fast convertors:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1298\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlist\u001b[39m(SLOW_TO_FAST_CONVERTERS\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1299\u001b[0m     )\n\u001b[1;32m   1301\u001b[0m converter_class \u001b[39m=\u001b[39m SLOW_TO_FAST_CONVERTERS[tokenizer_class_name]\n\u001b[0;32m-> 1303\u001b[0m \u001b[39mreturn\u001b[39;00m converter_class(transformer_tokenizer)\u001b[39m.\u001b[39mconverted()\n",
      "File \u001b[0;32m~/anaconda3/envs/cse6363/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py:445\u001b[0m, in \u001b[0;36mSpmConverter.__init__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    441\u001b[0m requires_backends(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mprotobuf\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    443\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39m*\u001b[39margs)\n\u001b[0;32m--> 445\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m sentencepiece_model_pb2 \u001b[39mas\u001b[39;00m model_pb2\n\u001b[1;32m    447\u001b[0m m \u001b[39m=\u001b[39m model_pb2\u001b[39m.\u001b[39mModelProto()\n\u001b[1;32m    448\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moriginal_tokenizer\u001b[39m.\u001b[39mvocab_file, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
      "File \u001b[0;32m~/anaconda3/envs/cse6363/lib/python3.11/site-packages/transformers/utils/sentencepiece_model_pb2.py:28\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39m# @@protoc_insertion_point(imports)\u001b[39;00m\n\u001b[1;32m     25\u001b[0m _sym_db \u001b[39m=\u001b[39m _symbol_database\u001b[39m.\u001b[39mDefault()\n\u001b[0;32m---> 28\u001b[0m DESCRIPTOR \u001b[39m=\u001b[39m _descriptor\u001b[39m.\u001b[39;49mFileDescriptor(\n\u001b[1;32m     29\u001b[0m     name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39msentencepiece_model.proto\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     30\u001b[0m     package\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39msentencepiece\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     31\u001b[0m     syntax\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mproto2\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     32\u001b[0m     serialized_options\u001b[39m=\u001b[39;49m\u001b[39mb\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mH\u001b[39;49m\u001b[39m\\003\u001b[39;49;00m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     33\u001b[0m     create_key\u001b[39m=\u001b[39;49m_descriptor\u001b[39m.\u001b[39;49m_internal_create_key,\n\u001b[1;32m     34\u001b[0m     serialized_pb\u001b[39m=\u001b[39;49m(\n\u001b[1;32m     35\u001b[0m         \u001b[39mb\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x19\u001b[39;49;00m\u001b[39msentencepiece_model.proto\u001b[39;49m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\r\u001b[39;49;00m\u001b[39msentencepiece\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\\xa1\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x0b\u001b[39;49;00m\u001b[39mTrainerSpec\u001b[39;49m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\r\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x05\u001b[39;49;00m\u001b[39minput\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m'\u001b[39;49m\n\u001b[1;32m     36\u001b[0m         \u001b[39mb\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\\x03\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\t\u001b[39;49;00m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x14\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x0c\u001b[39;49;00m\u001b[39minput_format\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\x07\u001b[39;49;00m\u001b[39m \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\t\u001b[39;49;00m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x14\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x0c\u001b[39;49;00m\u001b[39mmodel_prefix\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\x02\u001b[39;49;00m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m     37\u001b[0m         \u001b[39mb\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\t\u001b[39;49;00m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x41\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39mmodel_type\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\x03\u001b[39;49;00m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m     38\u001b[0m         \u001b[39mb\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\x0e\u001b[39;49;00m\u001b[39m\\x32\u001b[39;49;00m\u001b[39m$.sentencepiece.TrainerSpec.ModelType:\u001b[39;49m\u001b[39m\\x07\u001b[39;49;00m\u001b[39mUNIGRAM\u001b[39;49m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39mvocab_size\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\x04\u001b[39;49;00m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m     39\u001b[0m         \u001b[39mb\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\x05\u001b[39;49;00m\u001b[39m:\u001b[39;49m\u001b[39m\\x04\u001b[39;49;00m\u001b[39m\\x38\u001b[39;49;00m\u001b[39m\\x30\u001b[39;49;00m\u001b[39m\\x30\u001b[39;49;00m\u001b[39m\\x30\u001b[39;49;00m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x17\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x0f\u001b[39;49;00m\u001b[39m\\x61\u001b[39;49;00m\u001b[39m\\x63\u001b[39;49;00m\u001b[39m\\x63\u001b[39;49;00m\u001b[39m\\x65\u001b[39;49;00m\u001b[39mpt_language\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\x05\u001b[39;49;00m\u001b[39m \u001b[39;49m\u001b[39m\\x03\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\t\u001b[39;49;00m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m     40\u001b[0m         \u001b[39mb\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x15\u001b[39;49;00m\u001b[39mself_test_sample_size\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\x06\u001b[39;49;00m\u001b[39m \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\x05\u001b[39;49;00m\u001b[39m:\u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m\\x30\u001b[39;49;00m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\"\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x63\u001b[39;49;00m\u001b[39mharacter_coverage\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m'\u001b[39;49m\n\u001b[1;32m     41\u001b[0m         \u001b[39mb\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\x02\u001b[39;49;00m\u001b[39m:\u001b[39;49m\u001b[39m\\x06\u001b[39;49;00m\u001b[39m\\x30\u001b[39;49;00m\u001b[39m.9995\u001b[39;49m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x1e\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x13\u001b[39;49;00m\u001b[39minput_sentence_size\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\x0b\u001b[39;49;00m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m     42\u001b[0m         \u001b[39mb\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\x04\u001b[39;49;00m\u001b[39m:\u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m\\x30\u001b[39;49;00m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m$\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x16\u001b[39;49;00m\u001b[39mshuffle_input_sentence\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\x13\u001b[39;49;00m\u001b[39m \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\x08\u001b[39;49;00m\u001b[39m:\u001b[39;49m\u001b[39m\\x04\u001b[39;49;00m\u001b[39mtrue\u001b[39;49m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m     43\u001b[0m         \u001b[39mb\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x14\u001b[39;49;00m\u001b[39mmining_sentence_size\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\x0c\u001b[39;49;00m\u001b[39m \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\x05\u001b[39;49;00m\u001b[39m\\x42\u001b[39;49;00m\u001b[39m\\x02\u001b[39;49;00m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\"\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x16\u001b[39;49;00m\u001b[39mtraining_sentence_size\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\r\u001b[39;49;00m\u001b[39m'\u001b[39;49m\n\u001b[1;32m     44\u001b[0m         \u001b[39mb\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\x05\u001b[39;49;00m\u001b[39m\\x42\u001b[39;49;00m\u001b[39m\\x02\u001b[39;49;00m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x17\u001b[39;49;00m\u001b[39mseed_sentencepiece_size\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\x0e\u001b[39;49;00m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m     45\u001b[0m         \u001b[39mb\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\x05\u001b[39;49;00m\u001b[39m:\u001b[39;49m\u001b[39m\\x07\u001b[39;49;00m\u001b[39m\\x31\u001b[39;49;00m\u001b[39m\\x30\u001b[39;49;00m\u001b[39m\\x30\u001b[39;49;00m\u001b[39m\\x30\u001b[39;49;00m\u001b[39m\\x30\u001b[39;49;00m\u001b[39m\\x30\u001b[39;49;00m\u001b[39m\\x30\u001b[39;49;00m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x1e\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x10\u001b[39;49;00m\u001b[39mshrinking_factor\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\x0f\u001b[39;49;00m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m     46\u001b[0m         \u001b[39mb\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\x02\u001b[39;49;00m\u001b[39m:\u001b[39;49m\u001b[39m\\x04\u001b[39;49;00m\u001b[39m\\x30\u001b[39;49;00m\u001b[39m.75\u001b[39;49m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m!\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x13\u001b[39;49;00m\u001b[39mmax_sentence_length\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m     47\u001b[0m         \u001b[39mb\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\x05\u001b[39;49;00m\u001b[39m:\u001b[39;49m\u001b[39m\\x04\u001b[39;49;00m\u001b[39m\\x34\u001b[39;49;00m\u001b[39m\\x31\u001b[39;49;00m\u001b[39m\\x39\u001b[39;49;00m\u001b[39m\\x32\u001b[39;49;00m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x17\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x0b\u001b[39;49;00m\u001b[39mnum_threads\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\x10\u001b[39;49;00m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m     48\u001b[0m         \u001b[39mb\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\x05\u001b[39;49;00m\u001b[39m:\u001b[39;49m\u001b[39m\\x02\u001b[39;49;00m\u001b[39m\\x31\u001b[39;49;00m\u001b[39m\\x36\u001b[39;49;00m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x1d\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x12\u001b[39;49;00m\u001b[39mnum_sub_iterations\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\x11\u001b[39;49;00m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m     49\u001b[0m         \u001b[39mb\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\x05\u001b[39;49;00m\u001b[39m:\u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m\\x32\u001b[39;49;00m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m$\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x18\u001b[39;49;00m\u001b[39mmax_sentencepiece_length\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\x14\u001b[39;49;00m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m     50\u001b[0m         \u001b[39mb\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\x05\u001b[39;49;00m\u001b[39m:\u001b[39;49m\u001b[39m\\x02\u001b[39;49;00m\u001b[39m\\x31\u001b[39;49;00m\u001b[39m\\x36\u001b[39;49;00m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m%\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x17\u001b[39;49;00m\u001b[39msplit_by_unicode_script\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\x15\u001b[39;49;00m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m     51\u001b[0m         \u001b[39mb\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\x08\u001b[39;49;00m\u001b[39m:\u001b[39;49m\u001b[39m\\x04\u001b[39;49;00m\u001b[39mtrue\u001b[39;49m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x1d\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x0f\u001b[39;49;00m\u001b[39msplit_by_number\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\x17\u001b[39;49;00m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m     52\u001b[0m         \u001b[39mb\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\x08\u001b[39;49;00m\u001b[39m:\u001b[39;49m\u001b[39m\\x04\u001b[39;49;00m\u001b[39mtrue\u001b[39;49m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m!\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x13\u001b[39;49;00m\u001b[39msplit_by_whitespace\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\x16\u001b[39;49;00m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m     53\u001b[0m         \u001b[39mb\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\x08\u001b[39;49;00m\u001b[39m:\u001b[39;49m\u001b[39m\\x04\u001b[39;49;00m\u001b[39mtrue\u001b[39;49m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m)\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x1a\u001b[39;49;00m\u001b[39mtreat_whitespace_as_suffix\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m     54\u001b[0m         \u001b[39mb\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\x08\u001b[39;49;00m\u001b[39m:\u001b[39;49m\u001b[39m\\x05\u001b[39;49;00m\u001b[39m\\x66\u001b[39;49;00m\u001b[39m\\x61\u001b[39;49;00m\u001b[39mlse\u001b[39;49m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x1b\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x0c\u001b[39;49;00m\u001b[39msplit_digits\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\x19\u001b[39;49;00m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m     55\u001b[0m         \u001b[39mb\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\x08\u001b[39;49;00m\u001b[39m:\u001b[39;49m\u001b[39m\\x05\u001b[39;49;00m\u001b[39m\\x66\u001b[39;49;00m\u001b[39m\\x61\u001b[39;49;00m\u001b[39mlse\u001b[39;49m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x17\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x0f\u001b[39;49;00m\u001b[39m\\x63\u001b[39;49;00m\u001b[39montrol_symbols\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\x1e\u001b[39;49;00m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m     56\u001b[0m         \u001b[39mb\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\\x03\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\t\u001b[39;49;00m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x1c\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x14\u001b[39;49;00m\u001b[39muser_defined_symbols\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\x1f\u001b[39;49;00m\u001b[39m \u001b[39;49m\u001b[39m\\x03\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\t\u001b[39;49;00m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x16\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x0e\u001b[39;49;00m\u001b[39mrequired_chars\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m$\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m     57\u001b[0m         \u001b[39mb\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\t\u001b[39;49;00m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x1c\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\r\u001b[39;49;00m\u001b[39mbyte_fallback\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m# \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\x08\u001b[39;49;00m\u001b[39m:\u001b[39;49m\u001b[39m\\x05\u001b[39;49;00m\u001b[39m\\x66\u001b[39;49;00m\u001b[39m\\x61\u001b[39;49;00m\u001b[39mlse\u001b[39;49m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m+\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x1d\u001b[39;49;00m\u001b[39mvocabulary_output_piece_score\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m     58\u001b[0m         \u001b[39mb\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m  \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\x08\u001b[39;49;00m\u001b[39m:\u001b[39;49m\u001b[39m\\x04\u001b[39;49;00m\u001b[39mtrue\u001b[39;49m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x1e\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x10\u001b[39;49;00m\u001b[39mhard_vocab_limit\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m! \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\x08\u001b[39;49;00m\u001b[39m:\u001b[39;49m\u001b[39m\\x04\u001b[39;49;00m\u001b[39mtrue\u001b[39;49m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x1c\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\r\u001b[39;49;00m\u001b[39muse_all_vocab\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\"\u001b[39;49m\u001b[39m'\u001b[39;49m\n\u001b[1;32m     59\u001b[0m         \u001b[39mb\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\x08\u001b[39;49;00m\u001b[39m:\u001b[39;49m\u001b[39m\\x05\u001b[39;49;00m\u001b[39m\\x66\u001b[39;49;00m\u001b[39m\\x61\u001b[39;49;00m\u001b[39mlse\u001b[39;49m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x11\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x06\u001b[39;49;00m\u001b[39munk_id\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m( \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\x05\u001b[39;49;00m\u001b[39m:\u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m\\x30\u001b[39;49;00m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x11\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x06\u001b[39;49;00m\u001b[39m\\x62\u001b[39;49;00m\u001b[39mos_id\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m)\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m     60\u001b[0m         \u001b[39mb\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\x05\u001b[39;49;00m\u001b[39m:\u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m\\x31\u001b[39;49;00m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x11\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x06\u001b[39;49;00m\u001b[39m\\x65\u001b[39;49;00m\u001b[39mos_id\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m* \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\x05\u001b[39;49;00m\u001b[39m:\u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m\\x32\u001b[39;49;00m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x06\u001b[39;49;00m\u001b[39mpad_id\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m+\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m     61\u001b[0m         \u001b[39mb\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\x05\u001b[39;49;00m\u001b[39m:\u001b[39;49m\u001b[39m\\x02\u001b[39;49;00m\u001b[39m-1\u001b[39;49m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\t\u001b[39;49;00m\u001b[39munk_piece\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m- \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\t\u001b[39;49;00m\u001b[39m:\u001b[39;49m\u001b[39m\\x05\u001b[39;49;00m\u001b[39m<unk>\u001b[39;49m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x16\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\t\u001b[39;49;00m\u001b[39mbos_piece\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m.\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m     62\u001b[0m         \u001b[39mb\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\t\u001b[39;49;00m\u001b[39m:\u001b[39;49m\u001b[39m\\x03\u001b[39;49;00m\u001b[39m<s>\u001b[39;49m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x17\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\t\u001b[39;49;00m\u001b[39meos_piece\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m/ \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\t\u001b[39;49;00m\u001b[39m:\u001b[39;49m\u001b[39m\\x04\u001b[39;49;00m\u001b[39m</s>\u001b[39;49m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\t\u001b[39;49;00m\u001b[39mpad_piece\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\x30\u001b[39;49;00m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m     63\u001b[0m         \u001b[39mb\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\t\u001b[39;49;00m\u001b[39m:\u001b[39;49m\u001b[39m\\x05\u001b[39;49;00m\u001b[39m<pad>\u001b[39;49m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x1a\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x0b\u001b[39;49;00m\u001b[39munk_surface\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m, \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\t\u001b[39;49;00m\u001b[39m:\u001b[39;49m\u001b[39m\\x05\u001b[39;49;00m\u001b[39m \u001b[39;49m\u001b[39m\\xe2\u001b[39;49;00m\u001b[39m\\x81\u001b[39;49;00m\u001b[39m\\x87\u001b[39;49;00m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m     64\u001b[0m         \u001b[39mb\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m+\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x1c\u001b[39;49;00m\u001b[39mtrain_extremely_large_corpus\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\x31\u001b[39;49;00m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m     65\u001b[0m         \u001b[39mb\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\x08\u001b[39;49;00m\u001b[39m:\u001b[39;49m\u001b[39m\\x05\u001b[39;49;00m\u001b[39m\\x66\u001b[39;49;00m\u001b[39m\\x61\u001b[39;49;00m\u001b[39mlse\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m5\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\t\u001b[39;49;00m\u001b[39mModelType\u001b[39;49m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x0b\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x07\u001b[39;49;00m\u001b[39mUNIGRAM\u001b[39;49m\u001b[39m\\x10\u001b[39;49;00m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x07\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x03\u001b[39;49;00m\u001b[39m\\x42\u001b[39;49;00m\u001b[39mPE\u001b[39;49m\u001b[39m\\x10\u001b[39;49;00m\u001b[39m\\x02\u001b[39;49;00m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x08\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x04\u001b[39;49;00m\u001b[39mWORD\u001b[39;49m\u001b[39m\\x10\u001b[39;49;00m\u001b[39m\\x03\u001b[39;49;00m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x08\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x04\u001b[39;49;00m\u001b[39m\\x43\u001b[39;49;00m\u001b[39mHAR\u001b[39;49m\u001b[39m\\x10\u001b[39;49;00m\u001b[39m\\x04\u001b[39;49;00m\u001b[39m*\u001b[39;49m\u001b[39m\\t\u001b[39;49;00m\u001b[39m\\x08\u001b[39;49;00m\u001b[39m\\xc8\u001b[39;49;00m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m\\x10\u001b[39;49;00m\u001b[39m\\x80\u001b[39;49;00m\u001b[39m\\x80\u001b[39;49;00m\u001b[39m\\x80\u001b[39;49;00m\u001b[39m\\x80\u001b[39;49;00m\u001b[39m\\x02\u001b[39;49;00m\u001b[39m\"\u001b[39;49m\u001b[39m\\xd1\u001b[39;49;00m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x0e\u001b[39;49;00m\u001b[39mNormalizerSpec\u001b[39;49m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x0c\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x04\u001b[39;49;00m\u001b[39mname\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m'\u001b[39;49m\n\u001b[1;32m     66\u001b[0m         \u001b[39mb\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\t\u001b[39;49;00m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x1c\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x14\u001b[39;49;00m\u001b[39mprecompiled_charsmap\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\x02\u001b[39;49;00m\u001b[39m \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\x0c\u001b[39;49;00m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x1e\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x10\u001b[39;49;00m\u001b[39m\\x61\u001b[39;49;00m\u001b[39m\\x64\u001b[39;49;00m\u001b[39m\\x64\u001b[39;49;00m\u001b[39m_dummy_prefix\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\x03\u001b[39;49;00m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m     67\u001b[0m         \u001b[39mb\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\x08\u001b[39;49;00m\u001b[39m:\u001b[39;49m\u001b[39m\\x04\u001b[39;49;00m\u001b[39mtrue\u001b[39;49m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m&\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x18\u001b[39;49;00m\u001b[39mremove_extra_whitespaces\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\x04\u001b[39;49;00m\u001b[39m \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\x08\u001b[39;49;00m\u001b[39m:\u001b[39;49m\u001b[39m\\x04\u001b[39;49;00m\u001b[39mtrue\u001b[39;49m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m     68\u001b[0m         \u001b[39mb\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x65\u001b[39;49;00m\u001b[39mscape_whitespaces\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\x05\u001b[39;49;00m\u001b[39m \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\x08\u001b[39;49;00m\u001b[39m:\u001b[39;49m\u001b[39m\\x04\u001b[39;49;00m\u001b[39mtrue\u001b[39;49m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x1e\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x16\u001b[39;49;00m\u001b[39mnormalization_rule_tsv\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\x06\u001b[39;49;00m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m     69\u001b[0m         \u001b[39mb\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\t\u001b[39;49;00m\u001b[39m*\u001b[39;49m\u001b[39m\\t\u001b[39;49;00m\u001b[39m\\x08\u001b[39;49;00m\u001b[39m\\xc8\u001b[39;49;00m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m\\x10\u001b[39;49;00m\u001b[39m\\x80\u001b[39;49;00m\u001b[39m\\x80\u001b[39;49;00m\u001b[39m\\x80\u001b[39;49;00m\u001b[39m\\x80\u001b[39;49;00m\u001b[39m\\x02\u001b[39;49;00m\u001b[39m\"\u001b[39;49m\u001b[39my\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x0c\u001b[39;49;00m\u001b[39mSelfTestData\u001b[39;49m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x33\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x07\u001b[39;49;00m\u001b[39msamples\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m'\u001b[39;49m\n\u001b[1;32m     70\u001b[0m         \u001b[39mb\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\\x03\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\x0b\u001b[39;49;00m\u001b[39m\\x32\u001b[39;49;00m\u001b[39m\"\u001b[39;49m\u001b[39m.sentencepiece.SelfTestData.Sample\u001b[39;49m\u001b[39m\\x1a\u001b[39;49;00m\u001b[39m)\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x06\u001b[39;49;00m\u001b[39mSample\u001b[39;49m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\r\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x05\u001b[39;49;00m\u001b[39minput\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m'\u001b[39;49m\n\u001b[1;32m     71\u001b[0m         \u001b[39mb\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\t\u001b[39;49;00m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x10\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x08\u001b[39;49;00m\u001b[39m\\x65\u001b[39;49;00m\u001b[39mxpected\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\x02\u001b[39;49;00m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m     72\u001b[0m         \u001b[39mb\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\t\u001b[39;49;00m\u001b[39m*\u001b[39;49m\u001b[39m\\t\u001b[39;49;00m\u001b[39m\\x08\u001b[39;49;00m\u001b[39m\\xc8\u001b[39;49;00m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m\\x10\u001b[39;49;00m\u001b[39m\\x80\u001b[39;49;00m\u001b[39m\\x80\u001b[39;49;00m\u001b[39m\\x80\u001b[39;49;00m\u001b[39m\\x80\u001b[39;49;00m\u001b[39m\\x02\u001b[39;49;00m\u001b[39m\"\u001b[39;49m\u001b[39m\\xfe\u001b[39;49;00m\u001b[39m\\x03\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39mModelProto\u001b[39;49m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x37\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x06\u001b[39;49;00m\u001b[39mpieces\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m'\u001b[39;49m\n\u001b[1;32m     73\u001b[0m         \u001b[39mb\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\\x03\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\x0b\u001b[39;49;00m\u001b[39m\\x32\u001b[39;49;00m\u001b[39m'\u001b[39;49m\u001b[39m.sentencepiece.ModelProto.SentencePiece\u001b[39;49m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x30\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x0c\u001b[39;49;00m\u001b[39mtrainer_spec\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\x02\u001b[39;49;00m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m     74\u001b[0m         \u001b[39mb\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\x0b\u001b[39;49;00m\u001b[39m\\x32\u001b[39;49;00m\u001b[39m\\x1a\u001b[39;49;00m\u001b[39m.sentencepiece.TrainerSpec\u001b[39;49m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x36\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x0f\u001b[39;49;00m\u001b[39mnormalizer_spec\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\x03\u001b[39;49;00m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m     75\u001b[0m         \u001b[39mb\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\x0b\u001b[39;49;00m\u001b[39m\\x32\u001b[39;49;00m\u001b[39m\\x1d\u001b[39;49;00m\u001b[39m.sentencepiece.NormalizerSpec\u001b[39;49m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x33\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x0e\u001b[39;49;00m\u001b[39mself_test_data\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\x04\u001b[39;49;00m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m     76\u001b[0m         \u001b[39mb\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\x0b\u001b[39;49;00m\u001b[39m\\x32\u001b[39;49;00m\u001b[39m\\x1b\u001b[39;49;00m\u001b[39m.sentencepiece.SelfTestData\u001b[39;49m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x38\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x11\u001b[39;49;00m\u001b[39m\\x64\u001b[39;49;00m\u001b[39m\\x65\u001b[39;49;00m\u001b[39mnormalizer_spec\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\x05\u001b[39;49;00m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m     77\u001b[0m         \u001b[39mb\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\x0b\u001b[39;49;00m\u001b[39m\\x32\u001b[39;49;00m\u001b[39m\\x1d\u001b[39;49;00m\u001b[39m.sentencepiece.NormalizerSpec\u001b[39;49m\u001b[39m\\x1a\u001b[39;49;00m\u001b[39m\\xd2\u001b[39;49;00m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\r\u001b[39;49;00m\u001b[39mSentencePiece\u001b[39;49m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\r\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x05\u001b[39;49;00m\u001b[39mpiece\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m     78\u001b[0m         \u001b[39mb\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\t\u001b[39;49;00m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\r\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x05\u001b[39;49;00m\u001b[39mscore\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\x02\u001b[39;49;00m\u001b[39m \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\x02\u001b[39;49;00m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x42\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x04\u001b[39;49;00m\u001b[39mtype\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\x03\u001b[39;49;00m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m     79\u001b[0m         \u001b[39mb\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\x0e\u001b[39;49;00m\u001b[39m\\x32\u001b[39;49;00m\u001b[39m,.sentencepiece.ModelProto.SentencePiece.Type:\u001b[39;49m\u001b[39m\\x06\u001b[39;49;00m\u001b[39mNORMAL\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mT\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x04\u001b[39;49;00m\u001b[39mType\u001b[39;49m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x06\u001b[39;49;00m\u001b[39mNORMAL\u001b[39;49m\u001b[39m\\x10\u001b[39;49;00m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x0b\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x07\u001b[39;49;00m\u001b[39mUNKNOWN\u001b[39;49m\u001b[39m\\x10\u001b[39;49;00m\u001b[39m\\x02\u001b[39;49;00m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x0b\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x07\u001b[39;49;00m\u001b[39m\\x43\u001b[39;49;00m\u001b[39mONTROL\u001b[39;49m\u001b[39m\\x10\u001b[39;49;00m\u001b[39m\\x03\u001b[39;49;00m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x10\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x0c\u001b[39;49;00m\u001b[39mUSER_DEFINED\u001b[39;49m\u001b[39m\\x10\u001b[39;49;00m\u001b[39m\\x04\u001b[39;49;00m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x08\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x04\u001b[39;49;00m\u001b[39m\\x42\u001b[39;49;00m\u001b[39mYTE\u001b[39;49m\u001b[39m\\x10\u001b[39;49;00m\u001b[39m\\x06\u001b[39;49;00m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x06\u001b[39;49;00m\u001b[39mUNUSED\u001b[39;49m\u001b[39m\\x10\u001b[39;49;00m\u001b[39m\\x05\u001b[39;49;00m\u001b[39m*\u001b[39;49m\u001b[39m\\t\u001b[39;49;00m\u001b[39m\\x08\u001b[39;49;00m\u001b[39m\\xc8\u001b[39;49;00m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m\\x10\u001b[39;49;00m\u001b[39m\\x80\u001b[39;49;00m\u001b[39m\\x80\u001b[39;49;00m\u001b[39m\\x80\u001b[39;49;00m\u001b[39m\\x80\u001b[39;49;00m\u001b[39m\\x02\u001b[39;49;00m\u001b[39m*\u001b[39;49m\u001b[39m\\t\u001b[39;49;00m\u001b[39m\\x08\u001b[39;49;00m\u001b[39m\\xc8\u001b[39;49;00m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m\\x10\u001b[39;49;00m\u001b[39m\\x80\u001b[39;49;00m\u001b[39m\\x80\u001b[39;49;00m\u001b[39m\\x80\u001b[39;49;00m\u001b[39m\\x80\u001b[39;49;00m\u001b[39m\\x02\u001b[39;49;00m\u001b[39m\\x42\u001b[39;49;00m\u001b[39m\\x02\u001b[39;49;00m\u001b[39mH\u001b[39;49m\u001b[39m\\x03\u001b[39;49;00m\u001b[39m'\u001b[39;49m\n\u001b[1;32m     80\u001b[0m     ),\n\u001b[1;32m     81\u001b[0m )\n\u001b[1;32m     84\u001b[0m _TRAINERSPEC_MODELTYPE \u001b[39m=\u001b[39m _descriptor\u001b[39m.\u001b[39mEnumDescriptor(\n\u001b[1;32m     85\u001b[0m     name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModelType\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     86\u001b[0m     full_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msentencepiece.TrainerSpec.ModelType\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m     serialized_end\u001b[39m=\u001b[39m\u001b[39m1347\u001b[39m,\n\u001b[1;32m    128\u001b[0m )\n\u001b[1;32m    129\u001b[0m _sym_db\u001b[39m.\u001b[39mRegisterEnumDescriptor(_TRAINERSPEC_MODELTYPE)\n",
      "File \u001b[0;32m~/anaconda3/envs/cse6363/lib/python3.11/site-packages/google/protobuf/descriptor.py:1066\u001b[0m, in \u001b[0;36mFileDescriptor.__new__\u001b[0;34m(cls, name, package, options, serialized_options, serialized_pb, dependencies, public_dependencies, syntax, pool, create_key)\u001b[0m\n\u001b[1;32m   1058\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__new__\u001b[39m(\u001b[39mcls\u001b[39m, name, package, options\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1059\u001b[0m             serialized_options\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, serialized_pb\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1060\u001b[0m             dependencies\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, public_dependencies\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1063\u001b[0m   \u001b[39m# files, to register dynamic proto files and messages.\u001b[39;00m\n\u001b[1;32m   1064\u001b[0m   \u001b[39m# pylint: disable=g-explicit-bool-comparison\u001b[39;00m\n\u001b[1;32m   1065\u001b[0m   \u001b[39mif\u001b[39;00m serialized_pb:\n\u001b[0;32m-> 1066\u001b[0m     \u001b[39mreturn\u001b[39;00m _message\u001b[39m.\u001b[39;49mdefault_pool\u001b[39m.\u001b[39;49mAddSerializedFile(serialized_pb)\n\u001b[1;32m   1067\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1068\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m(FileDescriptor, \u001b[39mcls\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__new__\u001b[39m(\u001b[39mcls\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Couldn't build proto file into descriptor pool: duplicate file name sentencepiece_model.proto"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Evaluate this using PEGASUS\n",
    "pipe = pipeline(\"summarization\", model=\"google/pegasus-cnn_dailymail\", framework='pt')\n",
    "pipe_out = pipe(dataset_samsum[\"test\"][0][\"dialogue\"])\n",
    "print(\"Summary:\")\n",
    "print(pipe_out[0][\"summary_text\"].replace(\" .<n>\", \".\\n\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the entire test set\n",
    "\n",
    "We will need a way to compare the baseline PEGASUS model to the finetuned version. We'll create an evaluation loop for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def chunks(list_of_elements, batch_size):\n",
    "    for i in range(0, len(list_of_elements), batch_size):\n",
    "        yield list_of_elements[i : i + batch_size]\n",
    "\n",
    "def evaluate_summaries(dataset, metric, model, tokenizer,\n",
    "                       batch_size=16, device=device,\n",
    "                       column_text=\"article\", column_summary=\"highlights\"):\n",
    "    article_batches = list(chunks(dataset[column_text], batch_size))\n",
    "    target_batches = list(chunks(dataset[column_summary], batch_size))\n",
    "\n",
    "    for article_batch, target_batch in tqdm(\n",
    "        zip(article_batches, target_batches), total=len(article_batches)):\n",
    "\n",
    "        inputs = tokenizer(article_batch, max_length=1024, truncation=True,\n",
    "                        padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "        summaries = model.generate(input_ids=inputs[\"input_ids\"].to(device),\n",
    "                                   attention_mask=inputs[\"attention_mask\"].to(device),\n",
    "                                   length_penalty=0.8, num_beams=8, max_length=128)\n",
    "\n",
    "        decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True,\n",
    "                                              clean_up_tokenization_spaces=True)\n",
    "                             for s in summaries]\n",
    "\n",
    "        decoded_summaries = [d.replace(\"<n>\", \" \") for d in decoded_summaries]\n",
    "        \n",
    "    return metric.compute(predictions=decoded_summaries, references=target_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-25 21:23:15.223026: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Load the model directly\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "model_ckpt = \"ainize/bart-base-cnn\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 103/103 [04:32<00:00,  2.64s/it]\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "score = evaluate_summaries(dataset_samsum[\"test\"], rouge_metric, model,\n",
    "                           tokenizer, column_text=\"dialogue\",\n",
    "                           column_summary=\"summary\", batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>rougeLsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bart</th>\n",
       "      <td>0.383839</td>\n",
       "      <td>0.234127</td>\n",
       "      <td>0.347803</td>\n",
       "      <td>0.383839</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        rouge1    rouge2    rougeL  rougeLsum\n",
       "bart  0.383839  0.234127  0.347803   0.383839"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(score, index=[\"bart\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to fine tune this model, we need to be able to tokenize the data. We can also limit the lengths of each dialogue and summary to 1024 and 128, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5105be9987ce4156abf6ea545d0ece8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14732 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/anaconda3/envs/pytorch2/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3546: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09818c10ecc24da2a5cf5be4a4397eea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/819 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a09e39a2fcb4af08dce11ec44195e25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/818 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def convert_examples_to_features(example_batch):\n",
    "    input_encodings = tokenizer(example_batch[\"dialogue\"], truncation=True,\n",
    "                                max_length=1024)\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        target_encodings = tokenizer(example_batch[\"summary\"], max_length=128,\n",
    "                                     truncation=True)\n",
    "\n",
    "    return {\"input_ids\": input_encodings[\"input_ids\"],\n",
    "            \"attention_mask\": input_encodings[\"attention_mask\"],\n",
    "            \"labels\": target_encodings[\"input_ids\"]}\n",
    "\n",
    "dataset_samsum_pt = dataset_samsum.map(convert_examples_to_features,\n",
    "                                       batched=True)\n",
    "\n",
    "columns = [\"input_ids\", \"labels\", \"attention_mask\"]\n",
    "dataset_samsum_pt.set_format(type=\"torch\", columns=columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing a batch of data\n",
    "\n",
    "When training `seq2seq` models, we need to apply \"teacher forcing\". The encoder will receive input tokens using the labels shifted by one as well as the encoder output. The prediction is then compared to the shifted labels to calculate the loss. To clarify, the decoder only sees the previous ground truth labels.\n",
    "\n",
    "`HuggingFace` provides a `DataCollatorForSeq2Seq` class that handles this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "seq2seq_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# Gradient accumulation saves memory by updating the model only every X batches\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"bart-samsum\", num_train_epochs=1, warmup_steps=500,\n",
    "    per_device_train_batch_size=1, per_device_eval_batch_size=1,\n",
    "    weight_decay=0.01, logging_steps=10, push_to_hub=False,\n",
    "    evaluation_strategy=\"steps\", eval_steps=500, save_steps=1e6,\n",
    "    gradient_accumulation_steps=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: dialogue, summary, id. If dialogue, summary, id are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "/home/alex/anaconda3/envs/pytorch2/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 14732\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 16\n",
      "  Total optimization steps = 920\n",
      "  Number of trainable parameters = 139420416\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33majdillhoff\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/alex/dev/teaching/CSE6363/natural_language_processing/wandb/run-20230425_213928-d01c6jmr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ajdillhoff/huggingface/runs/d01c6jmr' target=\"_blank\">bart-samsum</a></strong> to <a href='https://wandb.ai/ajdillhoff/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ajdillhoff/huggingface' target=\"_blank\">https://wandb.ai/ajdillhoff/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ajdillhoff/huggingface/runs/d01c6jmr' target=\"_blank\">https://wandb.ai/ajdillhoff/huggingface/runs/d01c6jmr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c6617ee37d946b6a8271965d6abc12f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/920 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.9587, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.01}\n",
      "{'loss': 2.7171, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.02}\n",
      "{'loss': 2.5365, 'learning_rate': 3e-06, 'epoch': 0.03}\n",
      "{'loss': 2.3373, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.04}\n",
      "{'loss': 2.245, 'learning_rate': 5e-06, 'epoch': 0.05}\n",
      "{'loss': 2.1459, 'learning_rate': 6e-06, 'epoch': 0.07}\n",
      "{'loss': 2.0281, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.08}\n",
      "{'loss': 1.916, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.09}\n",
      "{'loss': 2.0147, 'learning_rate': 9e-06, 'epoch': 0.1}\n",
      "{'loss': 1.9108, 'learning_rate': 1e-05, 'epoch': 0.11}\n",
      "{'loss': 1.968, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.12}\n",
      "{'loss': 1.8644, 'learning_rate': 1.2e-05, 'epoch': 0.13}\n",
      "{'loss': 1.8956, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.14}\n",
      "{'loss': 1.8825, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.15}\n",
      "{'loss': 1.8367, 'learning_rate': 1.5e-05, 'epoch': 0.16}\n",
      "{'loss': 1.9063, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.17}\n",
      "{'loss': 1.7779, 'learning_rate': 1.7000000000000003e-05, 'epoch': 0.18}\n",
      "{'loss': 1.898, 'learning_rate': 1.8e-05, 'epoch': 0.2}\n",
      "{'loss': 1.8972, 'learning_rate': 1.9e-05, 'epoch': 0.21}\n",
      "{'loss': 1.8138, 'learning_rate': 2e-05, 'epoch': 0.22}\n",
      "{'loss': 1.8477, 'learning_rate': 2.1e-05, 'epoch': 0.23}\n",
      "{'loss': 1.8053, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.24}\n",
      "{'loss': 1.7627, 'learning_rate': 2.3000000000000003e-05, 'epoch': 0.25}\n",
      "{'loss': 1.8004, 'learning_rate': 2.4e-05, 'epoch': 0.26}\n",
      "{'loss': 1.7135, 'learning_rate': 2.5e-05, 'epoch': 0.27}\n",
      "{'loss': 1.8897, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.28}\n",
      "{'loss': 1.6728, 'learning_rate': 2.7000000000000002e-05, 'epoch': 0.29}\n",
      "{'loss': 1.7494, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.3}\n",
      "{'loss': 1.8096, 'learning_rate': 2.9e-05, 'epoch': 0.31}\n",
      "{'loss': 1.7087, 'learning_rate': 3e-05, 'epoch': 0.33}\n",
      "{'loss': 1.8226, 'learning_rate': 3.1e-05, 'epoch': 0.34}\n",
      "{'loss': 1.665, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.35}\n",
      "{'loss': 1.828, 'learning_rate': 3.3e-05, 'epoch': 0.36}\n",
      "{'loss': 1.7372, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.37}\n",
      "{'loss': 1.8032, 'learning_rate': 3.5e-05, 'epoch': 0.38}\n",
      "{'loss': 1.7394, 'learning_rate': 3.6e-05, 'epoch': 0.39}\n",
      "{'loss': 1.7557, 'learning_rate': 3.7e-05, 'epoch': 0.4}\n",
      "{'loss': 1.8106, 'learning_rate': 3.8e-05, 'epoch': 0.41}\n",
      "{'loss': 1.7554, 'learning_rate': 3.9000000000000006e-05, 'epoch': 0.42}\n",
      "{'loss': 1.7552, 'learning_rate': 4e-05, 'epoch': 0.43}\n",
      "{'loss': 1.772, 'learning_rate': 4.1e-05, 'epoch': 0.45}\n",
      "{'loss': 1.7465, 'learning_rate': 4.2e-05, 'epoch': 0.46}\n",
      "{'loss': 1.6667, 'learning_rate': 4.3e-05, 'epoch': 0.47}\n",
      "{'loss': 1.7713, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.48}\n",
      "{'loss': 1.7338, 'learning_rate': 4.5e-05, 'epoch': 0.49}\n",
      "{'loss': 1.7617, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.5}\n",
      "{'loss': 1.6181, 'learning_rate': 4.7e-05, 'epoch': 0.51}\n",
      "{'loss': 1.7016, 'learning_rate': 4.8e-05, 'epoch': 0.52}\n",
      "{'loss': 1.6852, 'learning_rate': 4.9e-05, 'epoch': 0.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: dialogue, summary, id. If dialogue, summary, id are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 818\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7254, 'learning_rate': 5e-05, 'epoch': 0.54}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93b9ef3a05e84ef4869a959a7bcc13ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/818 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5479345321655273, 'eval_runtime': 11.685, 'eval_samples_per_second': 70.004, 'eval_steps_per_second': 70.004, 'epoch': 0.54}\n",
      "{'loss': 1.8183, 'learning_rate': 4.880952380952381e-05, 'epoch': 0.55}\n",
      "{'loss': 1.7176, 'learning_rate': 4.761904761904762e-05, 'epoch': 0.56}\n",
      "{'loss': 1.7989, 'learning_rate': 4.642857142857143e-05, 'epoch': 0.58}\n",
      "{'loss': 1.7256, 'learning_rate': 4.523809523809524e-05, 'epoch': 0.59}\n",
      "{'loss': 1.7455, 'learning_rate': 4.404761904761905e-05, 'epoch': 0.6}\n",
      "{'loss': 1.6998, 'learning_rate': 4.2857142857142856e-05, 'epoch': 0.61}\n",
      "{'loss': 1.6311, 'learning_rate': 4.166666666666667e-05, 'epoch': 0.62}\n",
      "{'loss': 1.6529, 'learning_rate': 4.047619047619048e-05, 'epoch': 0.63}\n",
      "{'loss': 1.6794, 'learning_rate': 3.928571428571429e-05, 'epoch': 0.64}\n",
      "{'loss': 1.6679, 'learning_rate': 3.809523809523809e-05, 'epoch': 0.65}\n",
      "{'loss': 1.5988, 'learning_rate': 3.690476190476191e-05, 'epoch': 0.66}\n",
      "{'loss': 1.6306, 'learning_rate': 3.571428571428572e-05, 'epoch': 0.67}\n",
      "{'loss': 1.6587, 'learning_rate': 3.4523809523809526e-05, 'epoch': 0.68}\n",
      "{'loss': 1.6594, 'learning_rate': 3.3333333333333335e-05, 'epoch': 0.7}\n",
      "{'loss': 1.6443, 'learning_rate': 3.2142857142857144e-05, 'epoch': 0.71}\n",
      "{'loss': 1.6866, 'learning_rate': 3.095238095238095e-05, 'epoch': 0.72}\n",
      "{'loss': 1.6474, 'learning_rate': 2.9761904761904762e-05, 'epoch': 0.73}\n",
      "{'loss': 1.6495, 'learning_rate': 2.857142857142857e-05, 'epoch': 0.74}\n",
      "{'loss': 1.7268, 'learning_rate': 2.7380952380952383e-05, 'epoch': 0.75}\n",
      "{'loss': 1.6488, 'learning_rate': 2.6190476190476192e-05, 'epoch': 0.76}\n",
      "{'loss': 1.6696, 'learning_rate': 2.5e-05, 'epoch': 0.77}\n",
      "{'loss': 1.613, 'learning_rate': 2.380952380952381e-05, 'epoch': 0.78}\n",
      "{'loss': 1.6544, 'learning_rate': 2.261904761904762e-05, 'epoch': 0.79}\n",
      "{'loss': 1.6332, 'learning_rate': 2.1428571428571428e-05, 'epoch': 0.8}\n",
      "{'loss': 1.6545, 'learning_rate': 2.023809523809524e-05, 'epoch': 0.81}\n",
      "{'loss': 1.5423, 'learning_rate': 1.9047619047619046e-05, 'epoch': 0.83}\n",
      "{'loss': 1.589, 'learning_rate': 1.785714285714286e-05, 'epoch': 0.84}\n",
      "{'loss': 1.6196, 'learning_rate': 1.6666666666666667e-05, 'epoch': 0.85}\n",
      "{'loss': 1.5883, 'learning_rate': 1.5476190476190476e-05, 'epoch': 0.86}\n",
      "{'loss': 1.6747, 'learning_rate': 1.4285714285714285e-05, 'epoch': 0.87}\n",
      "{'loss': 1.6532, 'learning_rate': 1.3095238095238096e-05, 'epoch': 0.88}\n",
      "{'loss': 1.6582, 'learning_rate': 1.1904761904761905e-05, 'epoch': 0.89}\n",
      "{'loss': 1.6189, 'learning_rate': 1.0714285714285714e-05, 'epoch': 0.9}\n",
      "{'loss': 1.6074, 'learning_rate': 9.523809523809523e-06, 'epoch': 0.91}\n",
      "{'loss': 1.618, 'learning_rate': 8.333333333333334e-06, 'epoch': 0.92}\n",
      "{'loss': 1.6733, 'learning_rate': 7.142857142857143e-06, 'epoch': 0.93}\n",
      "{'loss': 1.5747, 'learning_rate': 5.9523809523809525e-06, 'epoch': 0.94}\n",
      "{'loss': 1.6336, 'learning_rate': 4.7619047619047615e-06, 'epoch': 0.96}\n",
      "{'loss': 1.5957, 'learning_rate': 3.5714285714285714e-06, 'epoch': 0.97}\n",
      "{'loss': 1.5581, 'learning_rate': 2.3809523809523808e-06, 'epoch': 0.98}\n",
      "{'loss': 1.5907, 'learning_rate': 1.1904761904761904e-06, 'epoch': 0.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5999, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 844.0132, 'train_samples_per_second': 17.455, 'train_steps_per_second': 1.09, 'train_loss': 1.776882411086041, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=920, training_loss=1.776882411086041, metrics={'train_runtime': 844.0132, 'train_samples_per_second': 17.455, 'train_steps_per_second': 1.09, 'train_loss': 1.776882411086041, 'epoch': 1.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(model=model, args=training_args,\n",
    "                  tokenizer=tokenizer, data_collator=seq2seq_collator,\n",
    "                  train_dataset=dataset_samsum_pt[\"train\"],\n",
    "                  eval_dataset=dataset_samsum_pt[\"validation\"])\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 410/410 [02:40<00:00,  2.55it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>rougeLsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bart_finetuned</th>\n",
       "      <td>0.4375</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                rouge1    rouge2  rougeL  rougeLsum\n",
       "bart_finetuned  0.4375  0.133333    0.25       0.25"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate after finetuning\n",
    "score = evaluate_summaries(\n",
    "    dataset_samsum[\"test\"], rouge_metric, trainer.model, tokenizer,\n",
    "    batch_size=2, column_text=\"dialogue\", column_summary=\"summary\")\n",
    "pd.DataFrame(score, index=[f\"bart_finetuned\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = dataset_samsum[\"test\"][0][\"dialogue\"]\n",
    "reference = dataset_samsum[\"test\"][0][\"summary\"]\n",
    "\n",
    "inputs = tokenizer(sample_text, max_length=1024, truncation=True,\n",
    "                   padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "summaries = model.generate(input_ids=inputs[\"input_ids\"].to(device),\n",
    "                           attention_mask=inputs[\"attention_mask\"].to(\n",
    "    device),\n",
    "    length_penalty=0.8, num_beams=8, max_length=128)\n",
    "\n",
    "decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True,\n",
    "                                      clean_up_tokenization_spaces=True)\n",
    "                     for s in summaries]\n",
    "\n",
    "decoded_summaries = [d.replace(\"<n>\", \" \") for d in decoded_summaries]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Amanda can't find Betty's number.\"]\n"
     ]
    }
   ],
   "source": [
    "print(decoded_summaries)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
