{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline\n",
    "- [Text Preprocessing](#Text-Preprocessing)\n",
    "    - [Tokenization](#Tokenization)\n",
    "    - [Stopwords](#Stopwords)\n",
    "    - [Stemming](#Stemming)\n",
    "    - [Lemmatization](#Lemmatization)\n",
    "    - [Bag of Words](#Bag-of-Words)\n",
    "    - [TF-IDF](#TF-IDF)\n",
    "    - [Word Embeddings](#Word-Embeddings)\n",
    "    - [Word2Vec](#Word2Vec)\n",
    "    - [GloVe](#GloVe)\n",
    "    - [FastText](#FastText)\n",
    "    - [References](#References)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuggingFace\n",
    "\n",
    "[HuggingFace](https://huggingface.co/) is a library for machine learning that provides pre-trained models and datasets. It also provides tools for training and evaluating models. It is a great resource for NLP. We will use it to access some helpful tools as well as the IMDB dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (/home/alex/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db601cd11c9240e98540e8f525d987dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    unsupervised: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "imdb_dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# Print the length of the train and test sets\n",
    "print(imdb_dataset)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "train_dataset, valid_dataset = random_split(imdb_dataset[\"train\"], [20000, 5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I rented I AM CURIOUS-YELLOW from my video sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"I Am Curious: Yellow\" is a risible and preten...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If only to avoid making this type of film in t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This film was probably inspired by Godard's Ma...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Oh, brother...after hearing about this ridicul...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  I rented I AM CURIOUS-YELLOW from my video sto...      0\n",
       "1  \"I Am Curious: Yellow\" is a risible and preten...      0\n",
       "2  If only to avoid making this type of film in t...      0\n",
       "3  This film was probably inspired by Godard's Ma...      0\n",
       "4  Oh, brother...after hearing about this ridicul...      0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as numpy\n",
    "import pandas as pd\n",
    "\n",
    "imdb_dataset.set_format(type=\"pandas\")\n",
    "df = imdb_dataset[\"train\"][:]\n",
    "\n",
    "imdb_dataset.set_format(type=\"torch\")\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "Tokenization is the process of breaking a stream of text up into words, phrases, symbols, or other meaningful elements called tokens. Tokens can be individual words, phrases or even whole sentences. The list of tokens becomes input for further processing.\n",
    "\n",
    "## Character Tokenization\n",
    "\n",
    "Character tokenization is the simplest form of tokenization. It breaks the text into individual characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\x08', '\\t', '\\x10', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~', '\\x80', '\\x84', '\\x85', '\\x8d', '\\x8e', '\\x91', '\\x95', '\\x96', '\\x97', '\\x9a', '\\x9e', '\\xa0', '¡', '¢', '£', '¤', '¦', '§', '¨', '«', '\\xad', '®', '°', '³', '´', '·', 'º', '»', '½', '¾', '¿', 'À', 'Á', 'Ã', 'Ä', 'Å', 'È', 'É', 'Ê', 'Õ', 'Ø', 'Ü', 'ß', 'à', 'á', 'â', 'ã', 'ä', 'å', 'æ', 'ç', 'è', 'é', 'ê', 'ë', 'ì', 'í', 'î', 'ï', 'ð', 'ñ', 'ò', 'ó', 'ô', 'ö', 'ø', 'ù', 'ú', 'û', 'ü', 'ý', 'ō', '–', '‘', '’', '“', '”', '…', '₤', '\\uf0b7']\n",
      "I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it's not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn't have much of a plot.\n",
      "[44, 3, 85, 72, 81, 87, 72, 71, 3, 44, 3, 36, 48, 3, 38, 56, 53, 44, 50, 56, 54, 16, 60, 40, 47, 47, 50, 58, 3, 73, 85, 82, 80, 3, 80, 92, 3, 89, 76, 71, 72, 82, 3, 86, 87, 82, 85, 72, 3, 69, 72, 70, 68, 88, 86, 72, 3, 82, 73, 3, 68, 79, 79, 3, 87, 75, 72, 3, 70, 82, 81, 87, 85, 82, 89, 72, 85, 86, 92, 3, 87, 75, 68, 87, 3, 86, 88, 85, 85, 82, 88, 81, 71, 72, 71, 3, 76, 87, 3, 90, 75, 72, 81, 3, 76, 87, 3, 90, 68, 86, 3, 73, 76, 85, 86, 87, 3, 85, 72, 79, 72, 68, 86, 72, 71, 3, 76, 81, 3, 20, 28, 25, 26, 17, 3, 44, 3, 68, 79, 86, 82, 3, 75, 72, 68, 85, 71, 3, 87, 75, 68, 87, 3, 68, 87, 3, 73, 76, 85, 86, 87, 3, 76, 87, 3, 90, 68, 86, 3, 86, 72, 76, 93, 72, 71, 3, 69, 92, 3, 56, 17, 54, 17, 3, 70, 88, 86, 87, 82, 80, 86, 3, 76, 73, 3, 76, 87, 3, 72, 89, 72, 85, 3, 87, 85, 76, 72, 71, 3, 87, 82, 3, 72, 81, 87, 72, 85, 3, 87, 75, 76, 86, 3, 70, 82, 88, 81, 87, 85, 92, 15, 3, 87, 75, 72, 85, 72, 73, 82, 85, 72, 3, 69, 72, 76, 81, 74, 3, 68, 3, 73, 68, 81, 3, 82, 73, 3, 73, 76, 79, 80, 86, 3, 70, 82, 81, 86, 76, 71, 72, 85, 72, 71, 3, 5, 70, 82, 81, 87, 85, 82, 89, 72, 85, 86, 76, 68, 79, 5, 3, 44, 3, 85, 72, 68, 79, 79, 92, 3, 75, 68, 71, 3, 87, 82, 3, 86, 72, 72, 3, 87, 75, 76, 86, 3, 73, 82, 85, 3, 80, 92, 86, 72, 79, 73, 17, 31, 69, 85, 3, 18, 33, 31, 69, 85, 3, 18, 33, 55, 75, 72, 3, 83, 79, 82, 87, 3, 76, 86, 3, 70, 72, 81, 87, 72, 85, 72, 71, 3, 68, 85, 82, 88, 81, 71, 3, 68, 3, 92, 82, 88, 81, 74, 3, 54, 90, 72, 71, 76, 86, 75, 3, 71, 85, 68, 80, 68, 3, 86, 87, 88, 71, 72, 81, 87, 3, 81, 68, 80, 72, 71, 3, 47, 72, 81, 68, 3, 90, 75, 82, 3, 90, 68, 81, 87, 86, 3, 87, 82, 3, 79, 72, 68, 85, 81, 3, 72, 89, 72, 85, 92, 87, 75, 76, 81, 74, 3, 86, 75, 72, 3, 70, 68, 81, 3, 68, 69, 82, 88, 87, 3, 79, 76, 73, 72, 17, 3, 44, 81, 3, 83, 68, 85, 87, 76, 70, 88, 79, 68, 85, 3, 86, 75, 72, 3, 90, 68, 81, 87, 86, 3, 87, 82, 3, 73, 82, 70, 88, 86, 3, 75, 72, 85, 3, 68, 87, 87, 72, 81, 87, 76, 82, 81, 86, 3, 87, 82, 3, 80, 68, 78, 76, 81, 74, 3, 86, 82, 80, 72, 3, 86, 82, 85, 87, 3, 82, 73, 3, 71, 82, 70, 88, 80, 72, 81, 87, 68, 85, 92, 3, 82, 81, 3, 90, 75, 68, 87, 3, 87, 75, 72, 3, 68, 89, 72, 85, 68, 74, 72, 3, 54, 90, 72, 71, 72, 3, 87, 75, 82, 88, 74, 75, 87, 3, 68, 69, 82, 88, 87, 3, 70, 72, 85, 87, 68, 76, 81, 3, 83, 82, 79, 76, 87, 76, 70, 68, 79, 3, 76, 86, 86, 88, 72, 86, 3, 86, 88, 70, 75, 3, 68, 86, 3, 87, 75, 72, 3, 57, 76, 72, 87, 81, 68, 80, 3, 58, 68, 85, 3, 68, 81, 71, 3, 85, 68, 70, 72, 3, 76, 86, 86, 88, 72, 86, 3, 76, 81, 3, 87, 75, 72, 3, 56, 81, 76, 87, 72, 71, 3, 54, 87, 68, 87, 72, 86, 17, 3, 44, 81, 3, 69, 72, 87, 90, 72, 72, 81, 3, 68, 86, 78, 76, 81, 74, 3, 83, 82, 79, 76, 87, 76, 70, 76, 68, 81, 86, 3, 68, 81, 71, 3, 82, 85, 71, 76, 81, 68, 85, 92, 3, 71, 72, 81, 76, 93, 72, 81, 86, 3, 82, 73, 3, 54, 87, 82, 70, 78, 75, 82, 79, 80, 3, 68, 69, 82, 88, 87, 3, 87, 75, 72, 76, 85, 3, 82, 83, 76, 81, 76, 82, 81, 86, 3, 82, 81, 3, 83, 82, 79, 76, 87, 76, 70, 86, 15, 3, 86, 75, 72, 3, 75, 68, 86, 3, 86, 72, 91, 3, 90, 76, 87, 75, 3, 75, 72, 85, 3, 71, 85, 68, 80, 68, 3, 87, 72, 68, 70, 75, 72, 85, 15, 3, 70, 79, 68, 86, 86, 80, 68, 87, 72, 86, 15, 3, 68, 81, 71, 3, 80, 68, 85, 85, 76, 72, 71, 3, 80, 72, 81, 17, 31, 69, 85, 3, 18, 33, 31, 69, 85, 3, 18, 33, 58, 75, 68, 87, 3, 78, 76, 79, 79, 86, 3, 80, 72, 3, 68, 69, 82, 88, 87, 3, 44, 3, 36, 48, 3, 38, 56, 53, 44, 50, 56, 54, 16, 60, 40, 47, 47, 50, 58, 3, 76, 86, 3, 87, 75, 68, 87, 3, 23, 19, 3, 92, 72, 68, 85, 86, 3, 68, 74, 82, 15, 3, 87, 75, 76, 86, 3, 90, 68, 86, 3, 70, 82, 81, 86, 76, 71, 72, 85, 72, 71, 3, 83, 82, 85, 81, 82, 74, 85, 68, 83, 75, 76, 70, 17, 3, 53, 72, 68, 79, 79, 92, 15, 3, 87, 75, 72, 3, 86, 72, 91, 3, 68, 81, 71, 3, 81, 88, 71, 76, 87, 92, 3, 86, 70, 72, 81, 72, 86, 3, 68, 85, 72, 3, 73, 72, 90, 3, 68, 81, 71, 3, 73, 68, 85, 3, 69, 72, 87, 90, 72, 72, 81, 15, 3, 72, 89, 72, 81, 3, 87, 75, 72, 81, 3, 76, 87, 10, 86, 3, 81, 82, 87, 3, 86, 75, 82, 87, 3, 79, 76, 78, 72, 3, 86, 82, 80, 72, 3, 70, 75, 72, 68, 83, 79, 92, 3, 80, 68, 71, 72, 3, 83, 82, 85, 81, 82, 17, 3, 58, 75, 76, 79, 72, 3, 80, 92, 3, 70, 82, 88, 81, 87, 85, 92, 80, 72, 81, 3, 80, 76, 81, 71, 3, 73, 76, 81, 71, 3, 76, 87, 3, 86, 75, 82, 70, 78, 76, 81, 74, 15, 3, 76, 81, 3, 85, 72, 68, 79, 76, 87, 92, 3, 86, 72, 91, 3, 68, 81, 71, 3, 81, 88, 71, 76, 87, 92, 3, 68, 85, 72, 3, 68, 3, 80, 68, 77, 82, 85, 3, 86, 87, 68, 83, 79, 72, 3, 76, 81, 3, 54, 90, 72, 71, 76, 86, 75, 3, 70, 76, 81, 72, 80, 68, 17, 3, 40, 89, 72, 81, 3, 44, 81, 74, 80, 68, 85, 3, 37, 72, 85, 74, 80, 68, 81, 15, 3, 68, 85, 74, 88, 68, 69, 79, 92, 3, 87, 75, 72, 76, 85, 3, 68, 81, 86, 90, 72, 85, 3, 87, 82, 3, 74, 82, 82, 71, 3, 82, 79, 71, 3, 69, 82, 92, 3, 45, 82, 75, 81, 3, 41, 82, 85, 71, 15, 3, 75, 68, 71, 3, 86, 72, 91, 3, 86, 70, 72, 81, 72, 86, 3, 76, 81, 3, 75, 76, 86, 3, 73, 76, 79, 80, 86, 17, 31, 69, 85, 3, 18, 33, 31, 69, 85, 3, 18, 33, 44, 3, 71, 82, 3, 70, 82, 80, 80, 72, 81, 71, 3, 87, 75, 72, 3, 73, 76, 79, 80, 80, 68, 78, 72, 85, 86, 3, 73, 82, 85, 3, 87, 75, 72, 3, 73, 68, 70, 87, 3, 87, 75, 68, 87, 3, 68, 81, 92, 3, 86, 72, 91, 3, 86, 75, 82, 90, 81, 3, 76, 81, 3, 87, 75, 72, 3, 73, 76, 79, 80, 3, 76, 86, 3, 86, 75, 82, 90, 81, 3, 73, 82, 85, 3, 68, 85, 87, 76, 86, 87, 76, 70, 3, 83, 88, 85, 83, 82, 86, 72, 86, 3, 85, 68, 87, 75, 72, 85, 3, 87, 75, 68, 81, 3, 77, 88, 86, 87, 3, 87, 82, 3, 86, 75, 82, 70, 78, 3, 83, 72, 82, 83, 79, 72, 3, 68, 81, 71, 3, 80, 68, 78, 72, 3, 80, 82, 81, 72, 92, 3, 87, 82, 3, 69, 72, 3, 86, 75, 82, 90, 81, 3, 76, 81, 3, 83, 82, 85, 81, 82, 74, 85, 68, 83, 75, 76, 70, 3, 87, 75, 72, 68, 87, 72, 85, 86, 3, 76, 81, 3, 36, 80, 72, 85, 76, 70, 68, 17, 3, 44, 3, 36, 48, 3, 38, 56, 53, 44, 50, 56, 54, 16, 60, 40, 47, 47, 50, 58, 3, 76, 86, 3, 68, 3, 74, 82, 82, 71, 3, 73, 76, 79, 80, 3, 73, 82, 85, 3, 68, 81, 92, 82, 81, 72, 3, 90, 68, 81, 87, 76, 81, 74, 3, 87, 82, 3, 86, 87, 88, 71, 92, 3, 87, 75, 72, 3, 80, 72, 68, 87, 3, 68, 81, 71, 3, 83, 82, 87, 68, 87, 82, 72, 86, 3, 11, 81, 82, 3, 83, 88, 81, 3, 76, 81, 87, 72, 81, 71, 72, 71, 12, 3, 82, 73, 3, 54, 90, 72, 71, 76, 86, 75, 3, 70, 76, 81, 72, 80, 68, 17, 3, 37, 88, 87, 3, 85, 72, 68, 79, 79, 92, 15, 3, 87, 75, 76, 86, 3, 73, 76, 79, 80, 3, 71, 82, 72, 86, 81, 10, 87, 3, 75, 68, 89, 72, 3, 80, 88, 70, 75, 3, 82, 73, 3, 68, 3, 83, 79, 82, 87, 17]\n",
      "I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it's not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn't have much of a plot.\n"
     ]
    }
   ],
   "source": [
    "# Get unique characters that exist in the dataset\n",
    "unique_chars = set(' '.join(df[:]['text'].tolist()))\n",
    "\n",
    "# Sort the unique characters\n",
    "unique_chars = sorted(list(unique_chars))\n",
    "print(unique_chars)\n",
    "\n",
    "# Tokenize the first input\n",
    "input_text = df[:]['text'].tolist()[0]\n",
    "print(input_text)\n",
    "\n",
    "# Create a dictionary that maps unique characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(unique_chars)}\n",
    "idx2char = {i:u for i, u in enumerate(unique_chars)}\n",
    "\n",
    "# Create encoder and decoder functions\n",
    "encode = lambda s: [char2idx[c] for c in s] # String to list of indices\n",
    "decode = lambda s: ''.join([idx2char[c] for c in s]) # List of indices to string\n",
    "input_seq = encode(input_text)\n",
    "\n",
    "# Print the result of encoding and decoding the first input\n",
    "print(input_seq)\n",
    "print(decode(encode(input_text)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word-level Tokenization\n",
    "\n",
    "Word tokenization is the process of breaking a stream of text up into words. The list of tokens becomes input for further processing. Using words instead of characters means that our model doesn't have to learn the word itself.\n",
    "\n",
    "As opposed to character-level tokenization, word tokenization requires a vastly larger dictionary size. This is because the number of unique words in a corpus is much larger than the number of unique characters. In practice, a subset of the most common words is used to build the dictionary.\n",
    "\n",
    "## Word Tokenization with `PyTorch` and `torchtext`\n",
    "\n",
    "HuggingFace provides access to many popular datasets for NLP. The `torchtext` library provides a simple API for loading and processing text data. It includes a variety of datasets, tokenizers, and data iterators. Using these libraries, we will prepare a dataset as follows:\n",
    "\n",
    "2. Create a vocabulary using the `torchtext` library.\n",
    "3. Convert the text to a sequence of integers using the vocabulary.\n",
    "\n",
    "We will start off by creating our own custom tokenizer. However, there are many other tokenizers available in the `torchtext` library. For example, the `spacy` tokenizer is a popular choice. The `spacy` tokenizer is a rule-based tokenizer that uses the `spaCy` library to tokenize text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  69241\n"
     ]
    }
   ],
   "source": [
    "# Create a tokenizer which removes punctuation and special characters before splitting the text into words.\n",
    "# This tokenizer is from Sebastian Raschka's book \"Machine Learning with PyTorch and sci-kit learn\"\n",
    "import re\n",
    "from collections import Counter, OrderedDict\n",
    "\n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', '')\n",
    "    tokenized = text.split()\n",
    "\n",
    "    return tokenized\n",
    "\n",
    "# Create a counter object to count the number of times each word appears in the dataset\n",
    "counter = Counter()\n",
    "\n",
    "# Loop through each review and tokenize it\n",
    "for sample in train_dataset:\n",
    "    line = sample['text']\n",
    "    counter.update(tokenizer(line))\n",
    "\n",
    "print(\"Vocabulary size: \", len(counter))\n",
    "\n",
    "# Create an encoder\n",
    "from torchtext.vocab import vocab\n",
    "\n",
    "sorted_tokens = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
    "ordered_dict = OrderedDict(sorted_tokens)\n",
    "vocab = vocab(ordered_dict)\n",
    "\n",
    "# Insert <unk> and <pad> tokens for unknown items and padding when batching fixed sized sequences\n",
    "vocab.insert_token('<pad>', 0)\n",
    "vocab.insert_token('<unk>', 1)\n",
    "vocab.set_default_index(1)\n",
    "\n",
    "# Create a function to encode the text\n",
    "def encode_text(text):\n",
    "    return vocab.lookup_indices(tokenizer(text))\n",
    "\n",
    "# Create a function to decode the text\n",
    "def decode_text(encoded_text):\n",
    "    return vocab.lookup_tokens(encoded_text)\n",
    "\n",
    "# Create a function to encode the labels\n",
    "def encode_label(label):\n",
    "    return 1 if label == 'pos' else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample review:  I saw this movie recently because a friend brought it with him from NYC. After 30 minutes, I said to him,\" You've got to be kidding. Is this some sort of joke?\" He thought it was good. I told him that I thought it was probably one of the silliest movies ever made. \"What was it supposed to be?\" I asked. \"A propaganda movie made for children?\" The plot is stupid. The acting is the worst ever for most of the principals and frankly people who look at this sort of tripe and think it has anything to do with life, love or even afterlife, of which it offers an incredibly idiotic view...need some psychiatric help. Please, if someone tries to get you to stick this in your DVD or Video player, consider it like you would a virus introduced into your computer...it won't destroy your player but it will destroy your evening. If they had made Razzies in the '40s, this would have won in every category. (PS. It also goes under the dubious sobriquet of \"Stairway to Heaven.\")\n",
      "Encoded review:  [10, 217, 11, 18, 1033, 87, 4, 435, 793, 8, 17, 89, 38, 4695, 101, 921, 227, 10, 307, 6, 89, 22, 137, 186, 6, 29, 4509, 7, 11, 49, 433, 5, 951, 25, 193, 8, 14, 50, 10, 569, 89, 12, 10, 193, 8, 14, 240, 30, 5, 2, 14469, 100, 126, 91, 48, 14, 8, 423, 6, 29, 10, 1846, 4, 2421, 18, 91, 16, 413, 2, 114, 7, 382, 2, 111, 7, 2, 247, 126, 16, 90, 5, 2, 6515, 3, 2047, 77, 36, 169, 32, 11, 433, 5, 5087, 3, 102, 8, 46, 232, 6, 79, 17, 108, 115, 42, 59, 10587, 5, 62, 8, 1571, 35, 982, 3217, 645, 358, 49, 10662, 345, 589, 47, 277, 501, 6, 75, 22, 6, 1255, 11, 9, 129, 282, 42, 371, 1750, 1118, 8, 39, 22, 60, 4, 3257, 1732, 82, 129, 1181, 8, 379, 21, 2386, 129, 1750, 19, 8, 80, 2386, 129, 2177, 47, 33, 67, 91, 37124, 9, 2, 5170, 11, 60, 28, 379, 9, 176, 2378, 7205, 8, 83, 267, 456, 2, 6367, 37536, 5, 10392, 6, 1660]\n",
      "Decoded review:  ['i', 'saw', 'this', 'movie', 'recently', 'because', 'a', 'friend', 'brought', 'it', 'with', 'him', 'from', 'nyc', 'after', '30', 'minutes', 'i', 'said', 'to', 'him', 'you', 've', 'got', 'to', 'be', 'kidding', 'is', 'this', 'some', 'sort', 'of', 'joke', 'he', 'thought', 'it', 'was', 'good', 'i', 'told', 'him', 'that', 'i', 'thought', 'it', 'was', 'probably', 'one', 'of', 'the', 'silliest', 'movies', 'ever', 'made', 'what', 'was', 'it', 'supposed', 'to', 'be', 'i', 'asked', 'a', 'propaganda', 'movie', 'made', 'for', 'children', 'the', 'plot', 'is', 'stupid', 'the', 'acting', 'is', 'the', 'worst', 'ever', 'for', 'most', 'of', 'the', 'principals', 'and', 'frankly', 'people', 'who', 'look', 'at', 'this', 'sort', 'of', 'tripe', 'and', 'think', 'it', 'has', 'anything', 'to', 'do', 'with', 'life', 'love', 'or', 'even', 'afterlife', 'of', 'which', 'it', 'offers', 'an', 'incredibly', 'idiotic', 'view', 'need', 'some', 'psychiatric', 'help', 'please', 'if', 'someone', 'tries', 'to', 'get', 'you', 'to', 'stick', 'this', 'in', 'your', 'dvd', 'or', 'video', 'player', 'consider', 'it', 'like', 'you', 'would', 'a', 'virus', 'introduced', 'into', 'your', 'computer', 'it', 'won', 't', 'destroy', 'your', 'player', 'but', 'it', 'will', 'destroy', 'your', 'evening', 'if', 'they', 'had', 'made', 'razzies', 'in', 'the', '40s', 'this', 'would', 'have', 'won', 'in', 'every', 'category', 'ps', 'it', 'also', 'goes', 'under', 'the', 'dubious', 'sobriquet', 'of', 'stairway', 'to', 'heaven']\n"
     ]
    }
   ],
   "source": [
    "# Sample a random review and tokenize it\n",
    "sample_idx = torch.randint(len(train_dataset), size=(1,)).item()\n",
    "sample_review = train_dataset[sample_idx]['text']\n",
    "sample_label = train_dataset[sample_idx]['label']\n",
    "print(\"Sample review: \", sample_review)\n",
    "\n",
    "# Encode the review\n",
    "encoded_review = encode_text(sample_review)\n",
    "print(\"Encoded review: \", encoded_review)\n",
    "\n",
    "# Decode the review\n",
    "decoded_review = decode_text(encoded_review)\n",
    "print(\"Decoded review: \", decoded_review)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subword Tokenization\n",
    "\n",
    "Subword tokenizations combine the benefits of both character and word-level tokenization. These tokenizers are learned from the data. They are able to break down words into smaller parts, resulting in a smaller vocabulary size. It also allows the model to learn from words that it has not seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded review:  [40, 2497, 428, 3807, 2904, 780, 257, 1545, 3181, 340, 351, 683, 422, 19170, 13, 2293, 1542, 2431, 11, 314, 531, 284, 683, 553, 921, 1053, 1392, 284, 307, 26471, 13, 1148, 428, 617, 3297, 286, 9707, 1701, 679, 1807, 340, 373, 922, 13, 314, 1297, 683, 326, 314, 1807, 340, 373, 2192, 530, 286, 262, 49276, 6386, 6918, 1683, 925, 13, 366, 2061, 373, 340, 4385, 284, 307, 1701, 314, 1965, 13, 366, 32, 11613, 3807, 925, 329, 1751, 1701, 383, 7110, 318, 8531, 13, 383, 7205, 318, 262, 5290, 1683, 329, 749, 286, 262, 44998, 290, 17813, 661, 508, 804, 379, 428, 3297, 286, 1333, 431, 290, 892, 340, 468, 1997, 284, 466, 351, 1204, 11, 1842, 393, 772, 45076, 11, 286, 543, 340, 4394, 281, 8131, 4686, 16357, 1570, 986, 31227, 617, 19906, 1037, 13, 4222, 11, 611, 2130, 8404, 284, 651, 345, 284, 4859, 428, 287, 534, 12490, 393, 7623, 2137, 11, 2074, 340, 588, 345, 561, 257, 9471, 5495, 656, 534, 3644, 986, 270, 1839, 470, 4117, 534, 2137, 475, 340, 481, 4117, 534, 6180, 13, 1002, 484, 550, 925, 371, 8101, 444, 287, 262, 705, 1821, 82, 11, 428, 561, 423, 1839, 287, 790, 6536, 13, 357, 3705, 13, 632, 635, 2925, 739, 262, 25292, 27355, 380, 21108, 286, 366, 1273, 958, 1014, 284, 11225, 19570]\n",
      "Decoded review:  I saw this movie recently because a friend brought it with him from NYC. After 30 minutes, I said to him,\" You've got to be kidding. Is this some sort of joke?\" He thought it was good. I told him that I thought it was probably one of the silliest movies ever made. \"What was it supposed to be?\" I asked. \"A propaganda movie made for children?\" The plot is stupid. The acting is the worst ever for most of the principals and frankly people who look at this sort of tripe and think it has anything to do with life, love or even afterlife, of which it offers an incredibly idiotic view...need some psychiatric help. Please, if someone tries to get you to stick this in your DVD or Video player, consider it like you would a virus introduced into your computer...it won't destroy your player but it will destroy your evening. If they had made Razzies in the '40s, this would have won in every category. (PS. It also goes under the dubious sobriquet of \"Stairway to Heaven.\")\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# Encode the review\n",
    "encoded_review = enc.encode(sample_review)\n",
    "print(\"Encoded review: \", encoded_review)\n",
    "\n",
    "# Decode the review\n",
    "decoded_review = enc.decode(encoded_review)\n",
    "print(\"Decoded review: \", decoded_review)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopwords\n",
    "\n",
    "Stop words are words that are most common words in a language. For many NLP tasks, these words are not useful and can be removed from the text. NLTK has a list of stopwords for many languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/alex/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download the stopwords from NLTK\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Import the stopword list\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Print the first 10 stopwords\n",
    "stopwords.words('english')[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
